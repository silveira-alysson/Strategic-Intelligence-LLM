{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1SvXV-8DWWyInsVfGByJ0jUZoLnCFKPou",
      "authorship_tag": "ABX9TyOqg9zPBjIGnyIfDhbkB/4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silveira-alysson/Strategic-Intelligence-LLM/blob/main/DECODA_SignOfLife_24Jan24_v5aos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Active Learning Component**"
      ],
      "metadata": {
        "id": "_yHQLDKsbn5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs:\n",
        "1. Corpus (10k reports as a dictionary)\n",
        "2. List of reports that were used in the test set (manually coded)\n",
        "\n",
        "Process:\n",
        "1. Import corpus\n",
        "2. Exclude subsample used in manual labeling\n",
        "3. Encode remaining documents\n",
        "4. Calculate similarity with ontology\n",
        "5. Choose top and bottom 200 documents in terms of similarity\n",
        "\n",
        "Outputs:\n",
        "1. Tokened corpus (pickle) - Intermediary output\n",
        "2. Enconded corpus (pickle) - Intermediary output\n",
        "3. File for human disambiguation"
      ],
      "metadata": {
        "id": "LuPxSjzuG4zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers --quiet\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "u0Qp-KHO4T9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #import whole 10k\n",
        "  encoded_corpus= pickle.load(open('/work_bgfs/a/alysson/Embedded_10k_whole/cdict.pickle', \"rb\"))\n",
        "\n",
        "  #import test set to check overlap\n",
        "  test = pd.read_csv('/work_bgfs/a/alysson/Embedded_10k_whole/test_set_assension_numbers.csv')\n",
        "\n",
        "  # Drop observations that are part of the test set\n",
        "  for unwanted_key in test['Row Labels']: del encoded_corpus[unwanted_key]\n",
        "\n",
        "  #generate a random sample of 3000 keys\n",
        "  keys = list(pd.DataFrame(encoded_corpus.keys(), columns=['key']).sample(3000, random_state=17).key)\n",
        "\n",
        "  encoded_corpus =  {k:v for (k,v) in encoded_corpus.items() if k in keys}\n",
        "\n",
        "  #import sentence transformers\n",
        "  model_save_path = 'all-roberta-large-v1'\n",
        "  model = SentenceTransformer(model_save_path)\n",
        "\n",
        "  # encode the ontology\n",
        "  query_embedding = model.encode([\"introduced a new generation of products\",\n",
        "        \"extended the product range\", \"openeded new markets\",\n",
        "        \"entered a new technology field\" ,\"improved an existing product quality\",\n",
        "        \"improved production flexibility\", \"reduced production cost\",\n",
        "        \"improved yield or reduced material consumption\"])\n",
        "\n",
        "  # create data frame\n",
        "  df = pd.DataFrame(columns = ['key', 'sentence', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8'])\n",
        "  for k, v in encoded_corpus.items():\n",
        "    if v != []:\n",
        "      df1 = pd.DataFrame([np.repeat(k ,len(v[0])), v[0], *util.cos_sim(query_embedding, v[1]).numpy()]).T\n",
        "      df1.columns = ['key', 'sentence', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8']\n",
        "      df = pd.concat((df, df1), axis=0)\n",
        "\n",
        "  pickle.dump(df, open('/work_bgfs/a/alysson/Embedded_10k_whole/Sample3000DF.pickle', \"wb\"))\n"
      ],
      "metadata": {
        "id": "O4dIuLZYAxVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  df = pickle.load(open('/work_bgfs/a/alysson/Embedded_10k_whole/Sample3000DF.pickle', \"rb\"))\n",
        "  #create sentence id\n",
        "  df['sentenceID'] = (df.groupby(['key']).cumcount()).astype(int)\n",
        "\n",
        "  #drop duplicated sentences\n",
        "  df = df.drop_duplicates(subset='sentence', keep='first', inplace=False)\n",
        "\n",
        "  #sort by similarity score within categories and display top 500\n",
        "  topC1 = df.sort_values(['c1'],ascending=False).head(500).sample(100)\n",
        "  topC2 = df.sort_values(['c2'],ascending=False).head(500).sample(100)\n",
        "  topC3 = df.sort_values(['c3'],ascending=False).head(500).sample(100)\n",
        "  topC4 = df.sort_values(['c4'],ascending=False).head(500).sample(100)\n",
        "  topC5 = df.sort_values(['c5'],ascending=False).head(500).sample(100)\n",
        "  topC6 = df.sort_values(['c6'],ascending=False).head(500).sample(100)\n",
        "  topC7 = df.sort_values(['c7'],ascending=False).head(500).sample(100)\n",
        "  topC8 = df.sort_values(['c8'],ascending=False).head(500).sample(100)\n",
        "\n",
        "  topSentences = topC1.append(topC2).append(topC3).append(topC4).append(topC5).append(topC6).append(topC7).append(topC8)\n",
        "  pickle.dump(topSentences , open('/work_bgfs/a/alysson/Embedded_10k_whole/top500sample100sentencesPerCategory3000Sample.pickle', \"wb\"))\n",
        "  topSentences.to_excel('/work_bgfs/a/alysson/Embedded_10k_whole/top500sample100sentencesPerCategory3000Sample.xlsx')\n",
        "  pickle.dump(df, open('/work_bgfs/a/alysson/Embedded_10k_whole/finalSampleDF_V2.pickle', \"wb\"))"
      ],
      "metadata": {
        "id": "zZ3y46hm4L7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETukf6OPGi3P"
      },
      "outputs": [],
      "source": [
        "# Connect with Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download corpus\n",
        "!wget -nc -O cp_10k.pickle https://www.dropbox.com/s/3g8x0695zp6h5oc/cp_10k.pickle?dl=0"
      ],
      "metadata": {
        "id": "bUaZgv4oHpWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sentence transformers\n",
        "!pip install sentence-transformers --quiet"
      ],
      "metadata": {
        "id": "xFVDrxXUHy1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import packages\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "cTCz8M-TH4wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import corpus (Dictionary with Business section of 10k reports)\n",
        "cp = pickle.load(open('/content/cp_10k.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "hcy7_1ueH_NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check lenght of cp\n",
        "len(cp)"
      ],
      "metadata": {
        "id": "yzvGD4seKn5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import test set to check overlap\n",
        "test = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/manual_coding_firm_level.xlsx').fillna(0)"
      ],
      "metadata": {
        "id": "qz_tedPUISyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop observations that are part of the test set\n",
        "for unwanted_key in test['Row Labels']: del cp[unwanted_key]\n",
        "len(cp)"
      ],
      "metadata": {
        "id": "AeYL-kblLsU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenize sample\n",
        "tokenized_sample = {k :tokenize.sent_tokenize(v) for (k, v) in cp.items()}"
      ],
      "metadata": {
        "id": "OVyuK_NmJPtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenized sample\n",
        "pickle.dump(tokenized_sample, open('/content/drive/MyDrive/DCAST/Financial/Inputs/tokenized_corpus.pickle', 'wb'))\n"
      ],
      "metadata": {
        "id": "AA-r5NBZNP7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load transformer\n",
        "model_save_path = 'all-roberta-large-v1'\n",
        "model = SentenceTransformer(model_save_path)\n",
        "\n",
        "encoded_corpus = {k :model.encode(v) for (k, v) in tokenized_sample.items()}"
      ],
      "metadata": {
        "id": "GYTdwhBWI_eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save encoded corpus\n",
        "pickle.dump(encoded_corpus, open('/content/drive/MyDrive/DCAST/Financial/Inputs/encoded_corpus.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "q97WjQI3QYD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the ontology\n",
        "query_embedding = model.encode([\"introduced a new generation of products\",\n",
        "      \"extended the product range\", \"openeded new markets\",\n",
        "      \"entered a new technology field\" ,\"improved an existing product quality\",\n",
        "      \"improved production flexibility\", \"reduced production cost\",\n",
        "      \"improved yield or reduced material consumption\"])"
      ],
      "metadata": {
        "id": "NXFBjWxSQjKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data frame\n",
        "df = pd.DataFrame(columns = ['key', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8'])\n",
        "for k, v in encoded_corpus.items():\n",
        "  if v != []:\n",
        "    df1 = pd.DataFrame([np.repeat(k ,v.shape[0]), *util.cos_sim(query_embedding, v).numpy()]).T\n",
        "    df1.columns = ['key', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8']\n",
        "    df = pd.concat((df, df1), axis=0)"
      ],
      "metadata": {
        "id": "2XR3CC9-QqZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add sentences\n",
        "dfv = pd.DataFrame()\n",
        "for k, v in tokenized_sample.items():\n",
        "  if v != []:\n",
        "    dfv1 = pd.DataFrame(v)\n",
        "    dfv = pd.concat((dfv, dfv1), axis=0)"
      ],
      "metadata": {
        "id": "doC0a3hYQ11Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.concat([df, dfv], axis=1)\n",
        "df = df.rename(columns={0:'sentence'})"
      ],
      "metadata": {
        "id": "ZXAPKeGFRHCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate sentence ids\n",
        "df['sentenceID'] = (df.groupby(['key']).cumcount()).astype(int)"
      ],
      "metadata": {
        "id": "NKfI2mKBRLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save data frame\n",
        "pickle.dump(df, open('/content/drive/MyDrive/DCAST/Financial/Inputs/df.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "oBMeen1xRTzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#re-create sentence ids because pickled version was wrong\n",
        "df['sentenceID'] = (df.groupby(['key']).cumcount()).astype(int)"
      ],
      "metadata": {
        "id": "Xn0BLzJLRg3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,9):\n",
        "  print(len(df[df[\"c\"+str(i)]>0.47]))"
      ],
      "metadata": {
        "id": "pe8KojwtRxGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate excel sheet for human disambiguation\n",
        "subsample = pd.DataFrame(columns = df.columns)\n",
        "for i in range(1,9):\n",
        "  a = df[df[\"c\"+str(i)]>np.mean(df[\"c\"+str(i)])+(3*np.std(df[\"c\"+str(i)]))].reset_index(drop=True).sample(100)\n",
        "  subsample = pd.concat((subsample, a), axis=0)\n",
        "\n",
        "new_column = [np.repeat('c1', 100), np.repeat('c2', 100), np.repeat('c3', 100), np.repeat('c4', 100), np.repeat('c5', 100), np.repeat('c6', 100), np.repeat('c7', 100), np.repeat('c8', 100)]\n",
        "subsample['class'] = list(itertools.chain(*new_column))\n",
        "subsample = subsample.drop(['c1','c2','c3','c4','c5','c6','c7','c8'], axis = 1).reset_index(drop=True).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "n9IRfj2CS996"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save file for human disambiguation\n",
        "subsample.to_excel('/content/drive/MyDrive/MISQ/Intermediates/2.disambiguation_file.xlsx')"
      ],
      "metadata": {
        "id": "f_u78puyclMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Augmentation Component**"
      ],
      "metadata": {
        "id": "GoyqkRA8c1Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs:\n",
        "1. Disambiaguation_file.xlsx (humanly disambiguated examples)\n",
        "2. Augmentation prompt templates\n",
        "\n",
        "Process:\n",
        "1. Pass augmentation prompts and disambiguated sentences to the OpenAI API to get completions\n",
        "\n",
        "Output:\n",
        "1. Augmented data frame\n"
      ],
      "metadata": {
        "id": "PI9u24dvdEQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install and import packages\n",
        "!pip install openai --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "Qq3sSP7deXQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass Open AI API key\n",
        "api_key = open(\"/content/drive/MyDrive/DCAST/Financial/Inputs/openAI_api.txt\", \"r\")\n",
        "openai.api_key = api_key.readlines(1)[0]"
      ],
      "metadata": {
        "id": "bzLAnKIigQCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import disambuguated examples\n",
        "# disambiguated_file = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/2.disambiguation_file_13Oct23_v1aos.xlsx', index_col=0)\n",
        "\n",
        "# #filter augmentedDF to keep only the sentences labeled as positives in the human disambiguation component\n",
        "# #this ensures that the negatives generated by the augmentation component are actual negatives\n",
        "# disambiguated_file_positives = disambiguated_file[disambiguated_file['label']==1]\n",
        "\n",
        "import ast\n",
        "disambiguated_file_positives = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/2.disambiguation_file_13Oct23_v2aos.xlsx', index_col=0, sheet_name=\"Sheet2\")\n",
        "#disambiguated_file_positives = disambiguated_file_positives['d'].apply(ast.literal_eval).tolist()"
      ],
      "metadata": {
        "id": "4ntUA5tIecyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import templates that will serve as examples for the prompt-based augmentation\n",
        "templates = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/5.Augmentation_Prompt_Templates_10_Shots_6_Types_Corpus_Based_13Oct23_v2aos.xlsx')"
      ],
      "metadata": {
        "id": "hHQDSzn2Van4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate instructions and examples for each class\n",
        "\n",
        "#create empty dataframe\n",
        "augmentedDF = pd.DataFrame(columns=['key','sentenceID','anchor','category','typeAug','text'])\n",
        "\n",
        "prompts = disambiguated_file_positives['prompt'].apply(ast.literal_eval).tolist()\n",
        "\n",
        "\n",
        "contexts = \"\"\n",
        "for j in range(0, len(disambiguated_file_positives.sentence)):\n",
        "#for j in range(221, len(disambiguated_file_positives.sentence)):\n",
        "  anchor = str(disambiguated_file_positives.iloc[j,1])\n",
        "  c = [prompts[j]]\n",
        "  key = str(disambiguated_file_positives.iloc[j,0])\n",
        "  id = str(disambiguated_file_positives.iloc[j,2])\n",
        "  cat = str(disambiguated_file_positives.iloc[j,3])\n",
        "\n",
        "  for i in range(0, len(templates.Type0)):\n",
        "    typeAug = templates.iloc[i,0] + \"-\" + templates.iloc[i,1]\n",
        "    temp = templates[\"Text2\"].apply(ast.literal_eval)[i]\n",
        "    prompt = temp + c\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages=prompt\n",
        "    )\n",
        "    augmentedDF.loc[len(augmentedDF)] = [key, id, anchor, cat, typeAug, response['choices'][0]['message']['content']]\n",
        "    time.sleep(7)\n",
        "    if j % 10 == 0:\n",
        "      augmentedDF.to_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/6.augmentedDF_GPT4_24Oct23_v2aos.xlsx')\n",
        "\n",
        "augmentedDF.to_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/6.augmentedDF_GPT4_24Oct23_v2aos.xlsx')"
      ],
      "metadata": {
        "id": "N_85ox4pgpFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate final dataset**"
      ],
      "metadata": {
        "id": "IgAK4fyYklMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs:\n",
        "1. Augmented dataset\n",
        "2. Humanly disambiguated file\n",
        "\n",
        "Process:\n",
        "\n",
        "\n",
        "Output:\n",
        "1."
      ],
      "metadata": {
        "id": "4nM0YW4SlLBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import datasets\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GqHSFdx5k7Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load augmented dataset\n",
        "augmentedDF = pd.read_excel('//content/drive/MyDrive/DCAST/Financial/Inputs/6.augmentedDF_GPT4_24Oct23_v2aos.xlsx', index_col=0)"
      ],
      "metadata": {
        "id": "hD1mshdTlHor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create label column based on the augType\n",
        "w = pd.DataFrame.from_dict({'Entailment-Core args' : 1, 'Entailment-Redudancy': 2, 'Entailment-Nominalization': 3,\n",
        "                            'Contradiction-Lexical': 4, 'Contradiction-Negation': 5, 'Contradiction-Random': 6}\n",
        "                           , orient = 'index', columns=['classAug'])\n",
        "augmentedDF = pd.merge(augmentedDF, w, left_on= 'typeAug', right_index= True)"
      ],
      "metadata": {
        "id": "_wsK1hwzlUBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create ontology column\n",
        "o = pd.DataFrame.from_dict({\"c1\": \"introduced a new generation of products\", \"c2\":\"extended the product range\", \"c3\" : \"opened new markets\",\n",
        "      \"c4\":\"entered a new technology field\" , \"c5\": \"improved an existing product quality\", \"c6\": \"improved production flexibility\",\n",
        "      \"c7\": \"reduced production cost\", \"c8\": \"improved yield or reduced material consumption\"}, orient = 'index', columns=['ontology'])\n",
        "augmentedDF = pd.merge(augmentedDF, o, left_on= 'category', right_index= True)"
      ],
      "metadata": {
        "id": "xOre_l_AlZw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import human disambiguated file\n",
        "disamb =  pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/2.disambiguation_file_13Oct23_v2aos.xlsx', index_col=False).drop(columns=\"Unnamed: 0\", axis=1).rename(columns={'class':'category'})\n",
        "\n",
        "#generate unique id by merging sentenceID and class\n",
        "disamb['idCat'] = None\n",
        "for i in range(0, len(disamb)):\n",
        "  disamb['idCat'][i] = disamb['key'][i] +'-'+ str(int(disamb['sentenceID'][i])) + '-' + disamb['category'][i]"
      ],
      "metadata": {
        "id": "y0DxFkmelePC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate unique id by merging sentenceID and category\n",
        "augmentedDF['idCat'] = None\n",
        "for i in range(0, len(augmentedDF)):\n",
        "  augmentedDF['idCat'][i] = augmentedDF['key'][i] +'-'+ str(int(augmentedDF['sentenceID'][i])) + '-' + augmentedDF['category'][i]"
      ],
      "metadata": {
        "id": "246eyPPHlzye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter augmentedDF to keep only the sentences labeled as positives in the human disambiguation component\n",
        "#this ensures that the negatives generated by the augmentation component are actual negatives\n",
        "#positives = augmentedDF[augmentedDF['idCat'].isin(disamb[disamb['label not strict']==1]['idCat'].unique())]"
      ],
      "metadata": {
        "id": "6zbV7b_yl7Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keep on the necessary fields\n",
        "positives = augmentedDF[['idCat', 'text', 'ontology', 'classAug']].rename(columns={'text':'sentence'})\n",
        "\n",
        "#create label\n",
        "positives['label'] = np.where(positives['classAug']< 4, 1, 0)"
      ],
      "metadata": {
        "id": "9Pe4adOMl9Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate class 0 (original) for the original sentences when labelled positive\n",
        "#generate class 7 (original contradiction) when labeled negative\n",
        "disamb['classAug'] = [0 if i == 1 else 7 for i in disamb['label']]\n",
        "\n",
        "#add sentences labeled as negatives in the human disambiguation component as random negations\n",
        "negatives_and_originals = pd.merge(disamb[['idCat', 'sentence', 'category', 'classAug', 'label']], o, left_on = 'category', right_index = True).drop('category', axis=1)\n",
        "\n",
        "#re-order columsn to match positives\n",
        "negatives_and_originals = negatives_and_originals[['idCat', 'sentence', 'ontology', 'classAug', 'label']]\n"
      ],
      "metadata": {
        "id": "4TxeBX8Xl_V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate positive and negative examples\n",
        "finalDF = pd.concat([positives, negatives_and_originals], axis=0)"
      ],
      "metadata": {
        "id": "sa4twNkcmBTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete originals\n",
        "finalDF = finalDF[(finalDF['classAug']!=0) & (finalDF['classAug']!=7)]"
      ],
      "metadata": {
        "id": "LqBEY5nbRn75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save final dataset\n",
        "finalDF.to_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/7.finalDF_Augs_24Oct23.xlsx')"
      ],
      "metadata": {
        "id": "ly77SE3omIis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AWCL Component**"
      ],
      "metadata": {
        "id": "Lr1k7rWlmVu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install package\n",
        "!pip install sentence_transformers --quiet"
      ],
      "metadata": {
        "id": "WImjgUT3mksE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "45_2tpztmntw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function**"
      ],
      "metadata": {
        "id": "IeX_t6_PnHHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, List\n",
        "\n",
        "\n",
        "class InputExampleAWCL:\n",
        "    \"\"\"\n",
        "    Structure for one input example with texts, the label and a unique id\n",
        "    \"\"\"\n",
        "    def __init__(self, guid: str = '', texts: List[str] = None, classAug: Union[int, float] = 0, label: Union[int, float] = 0):\n",
        "        \"\"\"\n",
        "        Creates one InputExample with the given texts, guid and label\n",
        "        :param guid\n",
        "            id for the example\n",
        "        :param texts\n",
        "            the texts for the example.\n",
        "        :param label\n",
        "            the label for the example\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.texts = texts\n",
        "        self.classAug = classAug\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<InputExample> label: {}, texts: {}\".format(str(self.label), \"; \".join(self.texts))"
      ],
      "metadata": {
        "id": "b6ARzIZMnKAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I had to chenge the sentence tranformer model so it accepts the class of positive and negatives as inputs\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import stat\n",
        "from collections import OrderedDict\n",
        "from typing import List, Dict, Tuple, Iterable, Type, Union, Callable, Optional\n",
        "import requests\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import transformers\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_url, cached_download\n",
        "import torch\n",
        "from torch import nn, Tensor, device\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "from tqdm.autonotebook import trange\n",
        "import math\n",
        "import queue\n",
        "import tempfile\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "from sentence_transformers import __MODEL_HUB_ORGANIZATION__\n",
        "from sentence_transformers.evaluation import SentenceEvaluator\n",
        "from sentence_transformers.util import import_from_string, batch_to_device, fullname, snapshot_download\n",
        "from sentence_transformers.models import Transformer, Pooling, Dense\n",
        "from sentence_transformers.model_card_templates import ModelCardTemplate\n",
        "from sentence_transformers import __version__\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SentenceTransformerAWCL(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Loads or create a SentenceTransformer model, that can be used to map sentences / text to embeddings.\n",
        "    :param model_name_or_path: If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from Huggingface models repository with that name.\n",
        "    :param modules: This parameter can be used to create custom SentenceTransformer models from scratch.\n",
        "    :param device: Device (like 'cuda' / 'cpu') that should be used for computation. If None, checks if a GPU can be used.\n",
        "    :param cache_folder: Path to store models. Can be also set by SENTENCE_TRANSFORMERS_HOME enviroment variable.\n",
        "    :param use_auth_token: HuggingFace authentication token to download private models.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name_or_path: Optional[str] = None,\n",
        "                 modules: Optional[Iterable[nn.Module]] = None,\n",
        "                 device: Optional[str] = None,\n",
        "                 cache_folder: Optional[str] = None,\n",
        "                 use_auth_token: Union[bool, str, None] = None\n",
        "                 ):\n",
        "        self._model_card_vars = {}\n",
        "        self._model_card_text = None\n",
        "        self._model_config = {}\n",
        "\n",
        "        if cache_folder is None:\n",
        "            cache_folder = os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
        "            if cache_folder is None:\n",
        "                try:\n",
        "                    from torch.hub import _get_torch_home\n",
        "\n",
        "                    torch_cache_home = _get_torch_home()\n",
        "                except ImportError:\n",
        "                    torch_cache_home = os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n",
        "\n",
        "                cache_folder = os.path.join(torch_cache_home, 'sentence_transformers')\n",
        "\n",
        "        if model_name_or_path is not None and model_name_or_path != \"\":\n",
        "            logger.info(\"Load pretrained SentenceTransformer: {}\".format(model_name_or_path))\n",
        "\n",
        "            #Old models that don't belong to any organization\n",
        "            basic_transformer_models = ['albert-base-v1', 'albert-base-v2', 'albert-large-v1', 'albert-large-v2', 'albert-xlarge-v1', 'albert-xlarge-v2', 'albert-xxlarge-v1', 'albert-xxlarge-v2', 'bert-base-cased-finetuned-mrpc', 'bert-base-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'bert-base-multilingual-cased', 'bert-base-multilingual-uncased', 'bert-base-uncased', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking', 'bert-large-cased', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-uncased-whole-word-masking', 'bert-large-uncased', 'camembert-base', 'ctrl', 'distilbert-base-cased-distilled-squad', 'distilbert-base-cased', 'distilbert-base-german-cased', 'distilbert-base-multilingual-cased', 'distilbert-base-uncased-distilled-squad', 'distilbert-base-uncased-finetuned-sst-2-english', 'distilbert-base-uncased', 'distilgpt2', 'distilroberta-base', 'gpt2-large', 'gpt2-medium', 'gpt2-xl', 'gpt2', 'openai-gpt', 'roberta-base-openai-detector', 'roberta-base', 'roberta-large-mnli', 'roberta-large-openai-detector', 'roberta-large', 't5-11b', 't5-3b', 't5-base', 't5-large', 't5-small', 'transfo-xl-wt103', 'xlm-clm-ende-1024', 'xlm-clm-enfr-1024', 'xlm-mlm-100-1280', 'xlm-mlm-17-1280', 'xlm-mlm-en-2048', 'xlm-mlm-ende-1024', 'xlm-mlm-enfr-1024', 'xlm-mlm-enro-1024', 'xlm-mlm-tlm-xnli15-1024', 'xlm-mlm-xnli15-1024', 'xlm-roberta-base', 'xlm-roberta-large-finetuned-conll02-dutch', 'xlm-roberta-large-finetuned-conll02-spanish', 'xlm-roberta-large-finetuned-conll03-english', 'xlm-roberta-large-finetuned-conll03-german', 'xlm-roberta-large', 'xlnet-base-cased', 'xlnet-large-cased']\n",
        "\n",
        "            if os.path.exists(model_name_or_path):\n",
        "                #Load from path\n",
        "                model_path = model_name_or_path\n",
        "            else:\n",
        "                #Not a path, load from hub\n",
        "                if '\\\\' in model_name_or_path or model_name_or_path.count('/') > 1:\n",
        "                    raise ValueError(\"Path {} not found\".format(model_name_or_path))\n",
        "\n",
        "                if '/' not in model_name_or_path and model_name_or_path.lower() not in basic_transformer_models:\n",
        "                    # A model from sentence-transformers\n",
        "                    model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \"/\" + model_name_or_path\n",
        "\n",
        "                model_path = os.path.join(cache_folder, model_name_or_path.replace(\"/\", \"_\"))\n",
        "\n",
        "                if not os.path.exists(os.path.join(model_path, 'modules.json')):\n",
        "                    # Download from hub with caching\n",
        "                    snapshot_download(model_name_or_path,\n",
        "                                        cache_dir=cache_folder,\n",
        "                                        library_name='sentence-transformers',\n",
        "                                        library_version=__version__,\n",
        "                                        ignore_files=['flax_model.msgpack', 'rust_model.ot', 'tf_model.h5'],\n",
        "                                        use_auth_token=use_auth_token)\n",
        "\n",
        "            if os.path.exists(os.path.join(model_path, 'modules.json')):    #Load as SentenceTransformer model\n",
        "                modules = self._load_sbert_model(model_path)\n",
        "            else:   #Load with AutoModel\n",
        "                modules = self._load_auto_model(model_path)\n",
        "\n",
        "        if modules is not None and not isinstance(modules, OrderedDict):\n",
        "            modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\n",
        "\n",
        "        super().__init__(modules)\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            logger.info(\"Use pytorch device: {}\".format(device))\n",
        "\n",
        "        self._target_device = torch.device(device)\n",
        "\n",
        "\n",
        "    def encode(self, sentences: Union[str, List[str]],\n",
        "               batch_size: int = 32,\n",
        "               show_progress_bar: bool = None,\n",
        "               output_value: str = 'sentence_embedding',\n",
        "               convert_to_numpy: bool = True,\n",
        "               convert_to_tensor: bool = False,\n",
        "               device: str = None,\n",
        "               normalize_embeddings: bool = False) -> Union[List[Tensor], ndarray, Tensor]:\n",
        "        \"\"\"\n",
        "        Computes sentence embeddings\n",
        "        :param sentences: the sentences to embed\n",
        "        :param batch_size: the batch size used for the computation\n",
        "        :param show_progress_bar: Output a progress bar when encode sentences\n",
        "        :param output_value:  Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings. Set to None, to get all output values\n",
        "        :param convert_to_numpy: If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.\n",
        "        :param convert_to_tensor: If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy\n",
        "        :param device: Which torch.device to use for the computation\n",
        "        :param normalize_embeddings: If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.\n",
        "        :return:\n",
        "           By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if show_progress_bar is None:\n",
        "            show_progress_bar = (logger.getEffectiveLevel()==logging.INFO or logger.getEffectiveLevel()==logging.DEBUG)\n",
        "\n",
        "        if convert_to_tensor:\n",
        "            convert_to_numpy = False\n",
        "\n",
        "        if output_value != 'sentence_embedding':\n",
        "            convert_to_tensor = False\n",
        "            convert_to_numpy = False\n",
        "\n",
        "        input_was_string = False\n",
        "        if isinstance(sentences, str) or not hasattr(sentences, '__len__'): #Cast an individual sentence to a list with length 1\n",
        "            sentences = [sentences]\n",
        "            input_was_string = True\n",
        "\n",
        "        if device is None:\n",
        "            device = self._target_device\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "        all_embeddings = []\n",
        "        length_sorted_idx = np.argsort([-self._text_length(sen) for sen in sentences])\n",
        "        sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
        "\n",
        "        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=not show_progress_bar):\n",
        "            sentences_batch = sentences_sorted[start_index:start_index+batch_size]\n",
        "            features = self.tokenize(sentences_batch)\n",
        "            features = batch_to_device(features, device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out_features = self.forward(features)\n",
        "\n",
        "                if output_value == 'token_embeddings':\n",
        "                    embeddings = []\n",
        "                    for token_emb, attention in zip(out_features[output_value], out_features['attention_mask']):\n",
        "                        last_mask_id = len(attention)-1\n",
        "                        while last_mask_id > 0 and attention[last_mask_id].item() == 0:\n",
        "                            last_mask_id -= 1\n",
        "\n",
        "                        embeddings.append(token_emb[0:last_mask_id+1])\n",
        "                elif output_value is None:  #Return all outputs\n",
        "                    embeddings = []\n",
        "                    for sent_idx in range(len(out_features['sentence_embedding'])):\n",
        "                        row =  {name: out_features[name][sent_idx] for name in out_features}\n",
        "                        embeddings.append(row)\n",
        "                else:   #Sentence embeddings\n",
        "                    embeddings = out_features[output_value]\n",
        "                    embeddings = embeddings.detach()\n",
        "                    if normalize_embeddings:\n",
        "                        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "                    # fixes for #522 and #487 to avoid oom problems on gpu with large datasets\n",
        "                    if convert_to_numpy:\n",
        "                        embeddings = embeddings.cpu()\n",
        "\n",
        "                all_embeddings.extend(embeddings)\n",
        "\n",
        "        all_embeddings = [all_embeddings[idx] for idx in np.argsort(length_sorted_idx)]\n",
        "\n",
        "        if convert_to_tensor:\n",
        "            all_embeddings = torch.stack(all_embeddings)\n",
        "        elif convert_to_numpy:\n",
        "            all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n",
        "\n",
        "        if input_was_string:\n",
        "            all_embeddings = all_embeddings[0]\n",
        "\n",
        "        return all_embeddings\n",
        "\n",
        "\n",
        "    def start_multi_process_pool(self, target_devices: List[str] = None):\n",
        "        \"\"\"\n",
        "        Starts multi process to process the encoding with several, independent processes.\n",
        "        This method is recommended if you want to encode on multiple GPUs. It is advised\n",
        "        to start only one process per GPU. This method works together with encode_multi_process\n",
        "        :param target_devices: PyTorch target devices, e.g. cuda:0, cuda:1... If None, all available CUDA devices will be used\n",
        "        :return: Returns a dict with the target processes, an input queue and and output queue.\n",
        "        \"\"\"\n",
        "        if target_devices is None:\n",
        "            if torch.cuda.is_available():\n",
        "                target_devices = ['cuda:{}'.format(i) for i in range(torch.cuda.device_count())]\n",
        "            else:\n",
        "                logger.info(\"CUDA is not available. Start 4 CPU worker\")\n",
        "                target_devices = ['cpu']*4\n",
        "\n",
        "        logger.info(\"Start multi-process pool on devices: {}\".format(', '.join(map(str, target_devices))))\n",
        "\n",
        "        ctx = mp.get_context('spawn')\n",
        "        input_queue = ctx.Queue()\n",
        "        output_queue = ctx.Queue()\n",
        "        processes = []\n",
        "\n",
        "        for cuda_id in target_devices:\n",
        "            p = ctx.Process(target=SentenceTransformer._encode_multi_process_worker, args=(cuda_id, self, input_queue, output_queue), daemon=True)\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "\n",
        "        return {'input': input_queue, 'output': output_queue, 'processes': processes}\n",
        "\n",
        "    @staticmethod\n",
        "    def stop_multi_process_pool(pool):\n",
        "        \"\"\"\n",
        "        Stops all processes started with start_multi_process_pool\n",
        "        \"\"\"\n",
        "        for p in pool['processes']:\n",
        "            p.terminate()\n",
        "\n",
        "        for p in pool['processes']:\n",
        "            p.join()\n",
        "            p.close()\n",
        "\n",
        "        pool['input'].close()\n",
        "        pool['output'].close()\n",
        "\n",
        "    def encode_multi_process(self, sentences: List[str], pool: Dict[str, object], batch_size: int = 32, chunk_size: int = None):\n",
        "        \"\"\"\n",
        "        This method allows to run encode() on multiple GPUs. The sentences are chunked into smaller packages\n",
        "        and sent to individual processes, which encode these on the different GPUs. This method is only suitable\n",
        "        for encoding large sets of sentences\n",
        "        :param sentences: List of sentences\n",
        "        :param pool: A pool of workers started with SentenceTransformer.start_multi_process_pool\n",
        "        :param batch_size: Encode sentences with batch size\n",
        "        :param chunk_size: Sentences are chunked and sent to the individual processes. If none, it determine a sensible size.\n",
        "        :return: Numpy matrix with all embeddings\n",
        "        \"\"\"\n",
        "\n",
        "        if chunk_size is None:\n",
        "            chunk_size = min(math.ceil(len(sentences) / len(pool[\"processes\"]) / 10), 5000)\n",
        "\n",
        "        logger.debug(f\"Chunk data into {math.ceil(len(sentences) / chunk_size)} packages of size {chunk_size}\")\n",
        "\n",
        "        input_queue = pool['input']\n",
        "        last_chunk_id = 0\n",
        "        chunk = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            chunk.append(sentence)\n",
        "            if len(chunk) >= chunk_size:\n",
        "                input_queue.put([last_chunk_id, batch_size, chunk])\n",
        "                last_chunk_id += 1\n",
        "                chunk = []\n",
        "\n",
        "        if len(chunk) > 0:\n",
        "            input_queue.put([last_chunk_id, batch_size, chunk])\n",
        "            last_chunk_id += 1\n",
        "\n",
        "        output_queue = pool['output']\n",
        "        results_list = sorted([output_queue.get() for _ in range(last_chunk_id)], key=lambda x: x[0])\n",
        "        embeddings = np.concatenate([result[1] for result in results_list])\n",
        "        return embeddings\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_multi_process_worker(target_device: str, model, input_queue, results_queue):\n",
        "        \"\"\"\n",
        "        Internal working process to encode sentences in multi-process setup\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                id, batch_size, sentences = input_queue.get()\n",
        "                embeddings = model.encode(sentences, device=target_device,  show_progress_bar=False, convert_to_numpy=True, batch_size=batch_size)\n",
        "                results_queue.put([id, embeddings])\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "\n",
        "    def get_max_seq_length(self):\n",
        "        \"\"\"\n",
        "        Returns the maximal sequence length for input the model accepts. Longer inputs will be truncated\n",
        "        \"\"\"\n",
        "        if hasattr(self._first_module(), 'max_seq_length'):\n",
        "            return self._first_module().max_seq_length\n",
        "\n",
        "        return None\n",
        "\n",
        "    def tokenize(self, texts: Union[List[str], List[Dict], List[Tuple[str, str]]]):\n",
        "        \"\"\"\n",
        "        Tokenizes the texts\n",
        "        \"\"\"\n",
        "        return self._first_module().tokenize(texts)\n",
        "\n",
        "    def get_sentence_features(self, *features):\n",
        "        return self._first_module().get_sentence_features(*features)\n",
        "\n",
        "    def get_sentence_embedding_dimension(self):\n",
        "        for mod in reversed(self._modules.values()):\n",
        "            sent_embedding_dim_method = getattr(mod, \"get_sentence_embedding_dimension\", None)\n",
        "            if callable(sent_embedding_dim_method):\n",
        "                return sent_embedding_dim_method()\n",
        "        return None\n",
        "\n",
        "    def _first_module(self):\n",
        "        \"\"\"Returns the first module of this sequential embedder\"\"\"\n",
        "        return self._modules[next(iter(self._modules))]\n",
        "\n",
        "    def _last_module(self):\n",
        "        \"\"\"Returns the last module of this sequential embedder\"\"\"\n",
        "        return self._modules[next(reversed(self._modules))]\n",
        "\n",
        "    def save(self, path: str, model_name: Optional[str] = None, create_model_card: bool = True, train_datasets: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Saves all elements for this seq. sentence embedder into different sub-folders\n",
        "        :param path: Path on disc\n",
        "        :param model_name: Optional model name\n",
        "        :param create_model_card: If True, create a README.md with basic information about this model\n",
        "        :param train_datasets: Optional list with the names of the datasets used to to train the model\n",
        "        \"\"\"\n",
        "        if path is None:\n",
        "            return\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Save model to {}\".format(path))\n",
        "        modules_config = []\n",
        "\n",
        "        #Save some model info\n",
        "        if '__version__' not in self._model_config:\n",
        "            self._model_config['__version__'] = {\n",
        "                    'sentence_transformers': __version__,\n",
        "                    'transformers': transformers.__version__,\n",
        "                    'pytorch': torch.__version__,\n",
        "                }\n",
        "\n",
        "        with open(os.path.join(path, 'config_sentence_transformers.json'), 'w') as fOut:\n",
        "            json.dump(self._model_config, fOut, indent=2)\n",
        "\n",
        "        #Save modules\n",
        "        for idx, name in enumerate(self._modules):\n",
        "            module = self._modules[name]\n",
        "            if idx == 0 and isinstance(module, Transformer):    #Save transformer model in the main folder\n",
        "                model_path = path + \"/\"\n",
        "            else:\n",
        "                model_path = os.path.join(path, str(idx)+\"_\"+type(module).__name__)\n",
        "\n",
        "            os.makedirs(model_path, exist_ok=True)\n",
        "            module.save(model_path)\n",
        "            modules_config.append({'idx': idx, 'name': name, 'path': os.path.basename(model_path), 'type': type(module).__module__})\n",
        "\n",
        "        with open(os.path.join(path, 'modules.json'), 'w') as fOut:\n",
        "            json.dump(modules_config, fOut, indent=2)\n",
        "\n",
        "        # Create model card\n",
        "        if create_model_card:\n",
        "            self._create_model_card(path, model_name, train_datasets)\n",
        "\n",
        "    def _create_model_card(self, path: str, model_name: Optional[str] = None, train_datasets: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Create an automatic model and stores it in path\n",
        "        \"\"\"\n",
        "        if self._model_card_text is not None and len(self._model_card_text) > 0:\n",
        "            model_card = self._model_card_text\n",
        "        else:\n",
        "            tags = ModelCardTemplate.__TAGS__.copy()\n",
        "            model_card = ModelCardTemplate.__MODEL_CARD__\n",
        "\n",
        "            if len(self._modules) == 2 and isinstance(self._first_module(), Transformer) and isinstance(self._last_module(), Pooling) and self._last_module().get_pooling_mode_str() in ['cls', 'max', 'mean']:\n",
        "                pooling_module = self._last_module()\n",
        "                pooling_mode = pooling_module.get_pooling_mode_str()\n",
        "                model_card = model_card.replace(\"{USAGE_TRANSFORMERS_SECTION}\", ModelCardTemplate.__USAGE_TRANSFORMERS__)\n",
        "                pooling_fct_name, pooling_fct = ModelCardTemplate.model_card_get_pooling_function(pooling_mode)\n",
        "                model_card = model_card.replace(\"{POOLING_FUNCTION}\", pooling_fct).replace(\"{POOLING_FUNCTION_NAME}\", pooling_fct_name).replace(\"{POOLING_MODE}\", pooling_mode)\n",
        "                tags.append('transformers')\n",
        "\n",
        "            # Print full model\n",
        "            model_card = model_card.replace(\"{FULL_MODEL_STR}\", str(self))\n",
        "\n",
        "            # Add tags\n",
        "            model_card = model_card.replace(\"{TAGS}\", \"\\n\".join([\"- \"+t for t in tags]))\n",
        "\n",
        "            datasets_str = \"\"\n",
        "            if train_datasets is not None:\n",
        "                datasets_str = \"datasets:\\n\"+\"\\n\".join([\"- \" + d for d in train_datasets])\n",
        "            model_card = model_card.replace(\"{DATASETS}\", datasets_str)\n",
        "\n",
        "            # Add dim info\n",
        "            self._model_card_vars[\"{NUM_DIMENSIONS}\"] = self.get_sentence_embedding_dimension()\n",
        "\n",
        "            # Replace vars we created while using the model\n",
        "            for name, value in self._model_card_vars.items():\n",
        "                model_card = model_card.replace(name, str(value))\n",
        "\n",
        "            # Replace remaining vars with default values\n",
        "            for name, value in ModelCardTemplate.__DEFAULT_VARS__.items():\n",
        "                model_card = model_card.replace(name, str(value))\n",
        "\n",
        "        if model_name is not None:\n",
        "            model_card = model_card.replace(\"{MODEL_NAME}\", model_name.strip())\n",
        "\n",
        "        with open(os.path.join(path, \"README.md\"), \"w\", encoding='utf8') as fOut:\n",
        "            fOut.write(model_card.strip())\n",
        "\n",
        "    def save_to_hub(self,\n",
        "                    repo_name: str,\n",
        "                    organization: Optional[str] = None,\n",
        "                    private: Optional[bool] = None,\n",
        "                    commit_message: str = \"Add new SentenceTransformer model.\",\n",
        "                    local_model_path: Optional[str] = None,\n",
        "                    exist_ok: bool = False,\n",
        "                    replace_model_card: bool = False,\n",
        "                    train_datasets: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.\n",
        "        :param repo_name: Repository name for your model in the Hub.\n",
        "        :param organization:  Organization in which you want to push your model or tokenizer (you must be a member of this organization).\n",
        "        :param private: Set to true, for hosting a prive model\n",
        "        :param commit_message: Message to commit while pushing.\n",
        "        :param local_model_path: Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded\n",
        "        :param exist_ok: If true, saving to an existing repository is OK. If false, saving only to a new repository is possible\n",
        "        :param replace_model_card: If true, replace an existing model card in the hub with the automatically created model card\n",
        "        :param train_datasets: Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.\n",
        "        :return: The url of the commit of your model in the given repository.\n",
        "        \"\"\"\n",
        "        token = HfFolder.get_token()\n",
        "        if token is None:\n",
        "            raise ValueError(\"You must login to the Hugging Face hub on this computer by typing `transformers-cli login`.\")\n",
        "\n",
        "        if '/' in repo_name:\n",
        "            splits = repo_name.split('/', maxsplit=1)\n",
        "            if organization is None or organization == splits[0]:\n",
        "                organization = splits[0]\n",
        "                repo_name = splits[1]\n",
        "            else:\n",
        "                raise ValueError(\"You passed and invalid repository name: {}.\".format(repo_name))\n",
        "\n",
        "        endpoint = \"https://huggingface.co\"\n",
        "        repo_id = repo_name\n",
        "        if organization:\n",
        "          repo_id = f\"{organization}/{repo_id}\"\n",
        "        repo_url = HfApi(endpoint=endpoint).create_repo(\n",
        "                repo_id=repo_id,\n",
        "                token=token,\n",
        "                private=private,\n",
        "                repo_type=None,\n",
        "                exist_ok=exist_ok,\n",
        "            )\n",
        "        full_model_name = repo_url[len(endpoint)+1:].strip(\"/\")\n",
        "\n",
        "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "            # First create the repo (and clone its content if it's nonempty).\n",
        "            logger.info(\"Create repository and clone it if it exists\")\n",
        "            repo = Repository(tmp_dir, clone_from=repo_url)\n",
        "\n",
        "            # If user provides local files, copy them.\n",
        "            if local_model_path:\n",
        "                copy_tree(local_model_path, tmp_dir)\n",
        "            else:  # Else, save model directly into local repo.\n",
        "                create_model_card = replace_model_card or not os.path.exists(os.path.join(tmp_dir, 'README.md'))\n",
        "                self.save(tmp_dir, model_name=full_model_name, create_model_card=create_model_card, train_datasets=train_datasets)\n",
        "\n",
        "            #Find files larger 5M and track with git-lfs\n",
        "            large_files = []\n",
        "            for root, dirs, files in os.walk(tmp_dir):\n",
        "                for filename in files:\n",
        "                    file_path = os.path.join(root, filename)\n",
        "                    rel_path = os.path.relpath(file_path, tmp_dir)\n",
        "\n",
        "                    if os.path.getsize(file_path) > (5 * 1024 * 1024):\n",
        "                        large_files.append(rel_path)\n",
        "\n",
        "            if len(large_files) > 0:\n",
        "                logger.info(\"Track files with git lfs: {}\".format(\", \".join(large_files)))\n",
        "                repo.lfs_track(large_files)\n",
        "\n",
        "            logger.info(\"Push model to the hub. This might take a while\")\n",
        "            push_return = repo.push_to_hub(commit_message=commit_message)\n",
        "\n",
        "            def on_rm_error(func, path, exc_info):\n",
        "                # path contains the path of the file that couldn't be removed\n",
        "                # let's just assume that it's read-only and unlink it.\n",
        "                try:\n",
        "                    os.chmod(path, stat.S_IWRITE)\n",
        "                    os.unlink(path)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Remove .git folder. On Windows, the .git folder might be read-only and cannot be deleted\n",
        "            # Hence, try to set write permissions on error\n",
        "            try:\n",
        "                for f in os.listdir(tmp_dir):\n",
        "                    shutil.rmtree(os.path.join(tmp_dir, f), onerror=on_rm_error)\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Error when deleting temp folder: {}\".format(str(e)))\n",
        "                pass\n",
        "\n",
        "        return push_return\n",
        "\n",
        "    def smart_batching_collate(self, batch):\n",
        "        \"\"\"\n",
        "        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n",
        "        Here, batch is a list of tuples: [(tokens, label), ...]\n",
        "        :param batch:\n",
        "            a batch from a SmartBatchingDataset\n",
        "        :return:\n",
        "            a batch of tensors for the model\n",
        "        \"\"\"\n",
        "        num_texts = len(batch[0].texts)\n",
        "        texts = [[] for _ in range(num_texts)]\n",
        "        labels = []\n",
        "        classAugs = []\n",
        "\n",
        "        for example in batch:\n",
        "            for idx, text in enumerate(example.texts):\n",
        "                texts[idx].append(text)\n",
        "\n",
        "            labels.append(example.label)\n",
        "            classAugs.append(example.classAug)\n",
        "\n",
        "        labels = torch.tensor(labels)\n",
        "        classAugs = torch.tensor(classAugs)\n",
        "\n",
        "        sentence_features = []\n",
        "        for idx in range(num_texts):\n",
        "            tokenized = self.tokenize(texts[idx])\n",
        "            sentence_features.append(tokenized)\n",
        "\n",
        "        return sentence_features, classAugs, labels\n",
        "\n",
        "    def _text_length(self, text: Union[List[int], List[List[int]]]):\n",
        "        \"\"\"\n",
        "        Help function to get the length for the input text. Text can be either\n",
        "        a list of ints (which means a single text as input), or a tuple of list of ints\n",
        "        (representing several text inputs to the model).\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(text, dict):              #{key: value} case\n",
        "            return len(next(iter(text.values())))\n",
        "        elif not hasattr(text, '__len__'):      #Object has no len() method\n",
        "            return 1\n",
        "        elif len(text) == 0 or isinstance(text[0], int):    #Empty string or list of ints\n",
        "            return len(text)\n",
        "        else:\n",
        "            return sum([len(t) for t in text])      #Sum of length of individual strings\n",
        "\n",
        "    def fit(self,\n",
        "            train_objectives: Iterable[Tuple[DataLoader, nn.Module]],\n",
        "            evaluator: SentenceEvaluator = None,\n",
        "            epochs: int = 1,\n",
        "            steps_per_epoch = None,\n",
        "            scheduler: str = 'WarmupLinear',\n",
        "            warmup_steps: int = 10000,\n",
        "            optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n",
        "            optimizer_params : Dict[str, object]= {'lr': 2e-5},\n",
        "            weight_decay: float = 0.01,\n",
        "            evaluation_steps: int = 0,\n",
        "            output_path: str = None,\n",
        "            save_best_model: bool = True,\n",
        "            max_grad_norm: float = 1,\n",
        "            use_amp: bool = False,\n",
        "            callback: Callable[[float, int, int], None] = None,\n",
        "            show_progress_bar: bool = True,\n",
        "            checkpoint_path: str = None,\n",
        "            checkpoint_save_steps: int = 500,\n",
        "            checkpoint_save_total_limit: int = 0\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Train the model with the given training objective\n",
        "        Each training objective is sampled in turn for one batch.\n",
        "        We sample only as many batches from each objective as there are in the smallest one\n",
        "        to make sure of equal training with each dataset.\n",
        "        :param train_objectives: Tuples of (DataLoader, LossFunction). Pass more than one for multi-task learning\n",
        "        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n",
        "        :param epochs: Number of epochs for training\n",
        "        :param steps_per_epoch: Number of training steps per epoch. If set to None (default), one epoch is equal the DataLoader size from train_objectives.\n",
        "        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
        "        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n",
        "        :param optimizer_class: Optimizer\n",
        "        :param optimizer_params: Optimizer parameters\n",
        "        :param weight_decay: Weight decay for model parameters\n",
        "        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n",
        "        :param output_path: Storage path for the model and evaluation files\n",
        "        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n",
        "        :param max_grad_norm: Used for gradient normalization.\n",
        "        :param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n",
        "        :param callback: Callback function that is invoked after each evaluation.\n",
        "                It must accept the following three parameters in this order:\n",
        "                `score`, `epoch`, `steps`\n",
        "        :param show_progress_bar: If True, output a tqdm progress bar\n",
        "        :param checkpoint_path: Folder to save checkpoints during training\n",
        "        :param checkpoint_save_steps: Will save a checkpoint after so many steps\n",
        "        :param checkpoint_save_total_limit: Total number of checkpoints to store\n",
        "        \"\"\"\n",
        "\n",
        "        ##Add info to model card\n",
        "        #info_loss_functions = \"\\n\".join([\"- {} with {} training examples\".format(str(loss), len(dataloader)) for dataloader, loss in train_objectives])\n",
        "        info_loss_functions =  []\n",
        "        for dataloader, loss in train_objectives:\n",
        "            info_loss_functions.extend(ModelCardTemplate.get_train_objective_info(dataloader, loss))\n",
        "        info_loss_functions = \"\\n\\n\".join([text for text in info_loss_functions])\n",
        "\n",
        "        info_fit_parameters = json.dumps({\"evaluator\": fullname(evaluator), \"epochs\": epochs, \"steps_per_epoch\": steps_per_epoch, \"scheduler\": scheduler, \"warmup_steps\": warmup_steps, \"optimizer_class\": str(optimizer_class),  \"optimizer_params\": optimizer_params, \"weight_decay\": weight_decay, \"evaluation_steps\": evaluation_steps, \"max_grad_norm\": max_grad_norm }, indent=4, sort_keys=True)\n",
        "        self._model_card_text = None\n",
        "        self._model_card_vars['{TRAINING_SECTION}'] = ModelCardTemplate.__TRAINING_SECTION__.replace(\"{LOSS_FUNCTIONS}\", info_loss_functions).replace(\"{FIT_PARAMETERS}\", info_fit_parameters)\n",
        "\n",
        "        if use_amp:\n",
        "            from torch.cuda.amp import autocast\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        self.to(self._target_device)\n",
        "\n",
        "        dataloaders = [dataloader for dataloader, _ in train_objectives]\n",
        "\n",
        "        # Use smart batching\n",
        "        for dataloader in dataloaders:\n",
        "            dataloader.collate_fn = self.smart_batching_collate\n",
        "\n",
        "        loss_models = [loss for _, loss in train_objectives]\n",
        "        for loss_model in loss_models:\n",
        "            loss_model.to(self._target_device)\n",
        "\n",
        "        self.best_score = -9999999\n",
        "\n",
        "        if steps_per_epoch is None or steps_per_epoch == 0:\n",
        "            steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n",
        "\n",
        "        num_train_steps = int(steps_per_epoch * epochs)\n",
        "\n",
        "        # Prepare optimizers\n",
        "        optimizers = []\n",
        "        schedulers = []\n",
        "        for loss_model in loss_models:\n",
        "            param_optimizer = list(loss_model.named_parameters())\n",
        "\n",
        "            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "            ]\n",
        "\n",
        "            optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
        "            scheduler_obj = self._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n",
        "\n",
        "            optimizers.append(optimizer)\n",
        "            schedulers.append(scheduler_obj)\n",
        "\n",
        "        global_step = 0\n",
        "        data_iterators = [iter(dataloader) for dataloader in dataloaders]\n",
        "\n",
        "        num_train_objectives = len(train_objectives)\n",
        "\n",
        "        skip_scheduler = False\n",
        "        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n",
        "            training_steps = 0\n",
        "\n",
        "            for loss_model in loss_models:\n",
        "                loss_model.zero_grad()\n",
        "                loss_model.train()\n",
        "\n",
        "            for _ in trange(steps_per_epoch, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n",
        "                for train_idx in range(num_train_objectives):\n",
        "                    loss_model = loss_models[train_idx]\n",
        "                    optimizer = optimizers[train_idx]\n",
        "                    scheduler = schedulers[train_idx]\n",
        "                    data_iterator = data_iterators[train_idx]\n",
        "\n",
        "                    try:\n",
        "                        data = next(data_iterator)\n",
        "                    except StopIteration:\n",
        "                        data_iterator = iter(dataloaders[train_idx])\n",
        "                        data_iterators[train_idx] = data_iterator\n",
        "                        data = next(data_iterator)\n",
        "\n",
        "                    features, classAugs, labels = data\n",
        "\n",
        "                    classAugs = classAugs.to(self._target_device)\n",
        "                    labels = labels.to(self._target_device)\n",
        "                    features = list(map(lambda batch: batch_to_device(batch, self._target_device), features))\n",
        "\n",
        "                    if use_amp:\n",
        "                        with autocast():\n",
        "                            loss_value = loss_model(features, classAugs, labels)\n",
        "\n",
        "                        scale_before_step = scaler.get_scale()\n",
        "                        scaler.scale(loss_value).backward()\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                        skip_scheduler = scaler.get_scale() != scale_before_step\n",
        "                    else:\n",
        "                        loss_value = loss_model(features, classAugs, labels)\n",
        "                        loss_value.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n",
        "                        optimizer.step()\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    if not skip_scheduler:\n",
        "                        scheduler.step()\n",
        "\n",
        "                training_steps += 1\n",
        "                global_step += 1\n",
        "\n",
        "                if evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n",
        "                    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n",
        "\n",
        "                    for loss_model in loss_models:\n",
        "                        loss_model.zero_grad()\n",
        "                        loss_model.train()\n",
        "\n",
        "                if checkpoint_path is not None and checkpoint_save_steps is not None and checkpoint_save_steps > 0 and global_step % checkpoint_save_steps == 0:\n",
        "                    self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n",
        "\n",
        "            self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)\n",
        "\n",
        "        if evaluator is None and output_path is not None:   #No evaluator, but output path: save final model version\n",
        "            self.save(output_path)\n",
        "\n",
        "        if checkpoint_path is not None:\n",
        "            self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n",
        "\n",
        "\n",
        "    def evaluate(self, evaluator: SentenceEvaluator, output_path: str = None):\n",
        "        \"\"\"\n",
        "        Evaluate the model\n",
        "        :param evaluator:\n",
        "            the evaluator\n",
        "        :param output_path:\n",
        "            the evaluator can write the results to this path\n",
        "        \"\"\"\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "        return evaluator(self, output_path)\n",
        "\n",
        "    def _eval_during_training(self, evaluator, output_path, save_best_model, epoch, steps, callback):\n",
        "        \"\"\"Runs evaluation during the training\"\"\"\n",
        "        eval_path = output_path\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "            eval_path = os.path.join(output_path, \"eval\")\n",
        "            os.makedirs(eval_path, exist_ok=True)\n",
        "\n",
        "        if evaluator is not None:\n",
        "            score = evaluator(self, output_path=eval_path, epoch=epoch, steps=steps)\n",
        "            if callback is not None:\n",
        "                callback(score, epoch, steps)\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                if save_best_model:\n",
        "                    self.save(output_path)\n",
        "\n",
        "    def _save_checkpoint(self, checkpoint_path, checkpoint_save_total_limit, step):\n",
        "        # Store new checkpoint\n",
        "        self.save(os.path.join(checkpoint_path, str(step)))\n",
        "\n",
        "        # Delete old checkpoints\n",
        "        if checkpoint_save_total_limit is not None and checkpoint_save_total_limit > 0:\n",
        "            old_checkpoints = []\n",
        "            for subdir in os.listdir(checkpoint_path):\n",
        "                if subdir.isdigit():\n",
        "                    old_checkpoints.append({'step': int(subdir), 'path': os.path.join(checkpoint_path, subdir)})\n",
        "\n",
        "            if len(old_checkpoints) > checkpoint_save_total_limit:\n",
        "                old_checkpoints = sorted(old_checkpoints, key=lambda x: x['step'])\n",
        "                shutil.rmtree(old_checkpoints[0]['path'])\n",
        "\n",
        "    def _load_auto_model(self, model_name_or_path):\n",
        "        \"\"\"\n",
        "        Creates a simple Transformer + Mean Pooling model and returns the modules\n",
        "        \"\"\"\n",
        "        logger.warning(\"No sentence-transformers model found with name {}. Creating a new one with MEAN pooling.\".format(model_name_or_path))\n",
        "        transformer_model = Transformer(model_name_or_path)\n",
        "        pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), 'mean')\n",
        "        return [transformer_model, pooling_model]\n",
        "\n",
        "    def _load_sbert_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Loads a full sentence-transformers model\n",
        "        \"\"\"\n",
        "        # Check if the config_sentence_transformers.json file exists (exists since v2 of the framework)\n",
        "        config_sentence_transformers_json_path = os.path.join(model_path, 'config_sentence_transformers.json')\n",
        "        if os.path.exists(config_sentence_transformers_json_path):\n",
        "            with open(config_sentence_transformers_json_path) as fIn:\n",
        "                self._model_config = json.load(fIn)\n",
        "\n",
        "            if '__version__' in self._model_config and 'sentence_transformers' in self._model_config['__version__'] and self._model_config['__version__']['sentence_transformers'] > __version__:\n",
        "                logger.warning(\"You try to use a model that was created with version {}, however, your version is {}. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\\n\\n\\n\".format(self._model_config['__version__']['sentence_transformers'], __version__))\n",
        "\n",
        "        # Check if a readme exists\n",
        "        model_card_path = os.path.join(model_path, 'README.md')\n",
        "        if os.path.exists(model_card_path):\n",
        "            try:\n",
        "                with open(model_card_path, encoding='utf8') as fIn:\n",
        "                    self._model_card_text = fIn.read()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Load the modules of sentence transformer\n",
        "        modules_json_path = os.path.join(model_path, 'modules.json')\n",
        "        with open(modules_json_path) as fIn:\n",
        "            modules_config = json.load(fIn)\n",
        "\n",
        "        modules = OrderedDict()\n",
        "        for module_config in modules_config:\n",
        "            module_class = import_from_string(module_config['type'])\n",
        "            module = module_class.load(os.path.join(model_path, module_config['path']))\n",
        "            modules[module_config['name']] = module\n",
        "\n",
        "        return modules\n",
        "\n",
        "    @staticmethod\n",
        "    def load(input_path):\n",
        "        return SentenceTransformer(input_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_scheduler(optimizer, scheduler: str, warmup_steps: int, t_total: int):\n",
        "        \"\"\"\n",
        "        Returns the correct learning rate scheduler. Available scheduler: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
        "        \"\"\"\n",
        "        scheduler = scheduler.lower()\n",
        "        if scheduler == 'constantlr':\n",
        "            return transformers.get_constant_schedule(optimizer)\n",
        "        elif scheduler == 'warmupconstant':\n",
        "            return transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "        elif scheduler == 'warmuplinear':\n",
        "            return transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "        elif scheduler == 'warmupcosine':\n",
        "            return transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "        elif scheduler == 'warmupcosinewithhardrestarts':\n",
        "            return transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown scheduler {}\".format(scheduler))\n",
        "\n",
        "    @property\n",
        "    def device(self) -> device:\n",
        "        \"\"\"\n",
        "        Get torch.device from module, assuming that the whole module has one device.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return next(self.parameters()).device\n",
        "        except StopIteration:\n",
        "            # For nn.DataParallel compatibility in PyTorch 1.5\n",
        "\n",
        "            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n",
        "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
        "                return tuples\n",
        "\n",
        "            gen = self._named_members(get_members_fn=find_tensor_attributes)\n",
        "            first_tuple = next(gen)\n",
        "            return first_tuple[1].device\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        \"\"\"\n",
        "        Property to get the tokenizer that is used by this model\n",
        "        \"\"\"\n",
        "        return self._first_module().tokenizer\n",
        "\n",
        "    @tokenizer.setter\n",
        "    def tokenizer(self, value):\n",
        "        \"\"\"\n",
        "        Property to set the tokenizer that should be used by this model\n",
        "        \"\"\"\n",
        "        self._first_module().tokenizer = value\n",
        "\n",
        "    @property\n",
        "    def max_seq_length(self):\n",
        "        \"\"\"\n",
        "        Property to get the maximal input sequence length for the model. Longer inputs will be truncated.\n",
        "        \"\"\"\n",
        "        return self._first_module().max_seq_length\n",
        "\n",
        "    @max_seq_length.setter\n",
        "    def max_seq_length(self, value):\n",
        "        \"\"\"\n",
        "        Property to set the maximal input sequence length for the model. Longer inputs will be truncated.\n",
        "        \"\"\"\n",
        "        self._first_module().max_seq_length = value\n",
        "\n"
      ],
      "metadata": {
        "id": "DCzXtpRenME1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from typing import Iterable, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
        "\n",
        "\n",
        "class AWCLContrastiveLoss(nn.Module):\n",
        "    def __init__(self, model: SentenceTransformerAWCL, weights: torch.Tensor, margin: float = 0.5, size_average:bool = True, measure: str = \"cosine\"):\n",
        "        super(AWCLContrastiveLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.weights = weights\n",
        "        self.margin = margin\n",
        "        self.size_average = size_average\n",
        "        self.measure = measure\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], classAug: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
        "        assert len(reps) == 2\n",
        "        sentence, ontology = reps\n",
        "\n",
        "        if self.measure == \"cosine\":\n",
        "            dot_product = torch.sum(sentence * ontology, dim=1)\n",
        "            magnitudes = torch.sqrt(torch.sum(sentence.pow(2), dim=1) * torch.sum(ontology.pow(2), dim=1))\n",
        "            distance = 1 - (dot_product / magnitudes)\n",
        "\n",
        "        weight = self.weights[classAug]  # look up weight for each class\n",
        "\n",
        "        losses = 0.5 * (label.float() * distance.pow(2) + (1 - label).float() * F.relu(self.margin - distance).pow(2)) * weight\n",
        "\n",
        "        return losses.mean() if self.size_average else losses.sum()\n"
      ],
      "metadata": {
        "id": "IrXSMour0GGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tunning**"
      ],
      "metadata": {
        "id": "U4SCOsDTnVLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#finalDF = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/7.finalDF_24Oct23.xlsx', index_col = 0)\n",
        "finalDF = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/7.finalDF_OLD.xlsx', index_col = 0)"
      ],
      "metadata": {
        "id": "73wG9Q4e3n1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalDF.reset_index(inplace=True,drop=True)"
      ],
      "metadata": {
        "id": "LXBk9GDg3xB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfSamples = finalDF.reset_index(drop=True)\n",
        "ind = dfSamples.groupby(['ontology', 'label'], group_keys=False).apply(lambda x: x.sample(frac=0.7, random_state=17)).index\n",
        "train_df = dfSamples[dfSamples.index.isin(ind)]\n",
        "dev_df = dfSamples[~dfSamples.index.isin(ind)]"
      ],
      "metadata": {
        "id": "JqrToDZzobJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "#from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util\n",
        "from sentence_transformers import InputExample\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "model_name = 'all-mpnet-base-v2'\n",
        "train_batch_size = 64\n",
        "num_epochs = 2\n",
        "model_save_path = '/content/drive/MyDrive/DCAST/Financial/Outputs/fine_tuned_models/'+model_name+'-' + 'withWeights-V7-' +datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformerAWCL(model_name)\n",
        "\n",
        "train_samples = []\n",
        "dev_samples = []\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "        inp_example = InputExampleAWCL(texts=[row.sentence, row.ontology], classAug=row.classAug, label=row.label)\n",
        "        train_samples.append(inp_example)\n",
        "\n",
        "for index, row in dev_df.iterrows():\n",
        "        inp_example_dev = InputExample(texts=[row.sentence, row.ontology], label=row.label)\n",
        "        dev_samples.append(inp_example_dev)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "\n",
        "# weights is a tensor of shape (8,) containing the weight for each class\n",
        "# the order is as follows (original positive sentence, core arguments, redudancy, nominalization, lexical, negation, random, original negative sentence ) where first three are positives and the last three are negatives\n",
        "weights = torch.tensor([0.9,0.9,0.9,1.2,1.1,1.3]).to('cuda')\n",
        "train_loss = AWCLContrastiveLoss(model=model, weights=weights)\n",
        "\n",
        "# Development set: Measure correlation between cosine score and gold labels\n",
        "evaluator = BinaryClassificationEvaluator.from_input_examples(dev_samples, name='fine-tune-dev')\n",
        "\n",
        "# Configure the training. We skip evaluation in this example\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
        "\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          optimizer_class=torch.optim.AdamW,\n",
        "          output_path=model_save_path,\n",
        "          save_best_model=True)\n"
      ],
      "metadata": {
        "id": "A5YGVFdToe0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers --quiet"
      ],
      "metadata": {
        "id": "94RQB73P9-yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras --quiet"
      ],
      "metadata": {
        "id": "E_Id_qhd-HM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-encoding training set to define thresholds (mean + 3 std)**"
      ],
      "metadata": {
        "id": "chDPIbXFI3Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "FpZksckRKx9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pickle.load(open('/content/drive/MyDrive/DCAST/Financial/Inputs/Sample3000DF.pickle', \"rb\"))"
      ],
      "metadata": {
        "id": "TFxlfRnzh9q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[['key', 'sentence']].sample(100000, random_state=17)"
      ],
      "metadata": {
        "id": "5FrlR5zNitXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode test set\n",
        "\n",
        "model_name = 'all-mpnet-base-v2-withWeights-V7-2023-10-26_18-40'\n",
        "model_save_path = '/content/drive/MyDrive/DCAST/Financial/Outputs/fine_tuned_models/' + model_name\n",
        "\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "query_embedding = model.encode([\"introduced a new generation of products\",\n",
        "      \"extended the product range\", \"opened new markets\",\n",
        "      \"entered a new technology field\" ,\"improved an existing product quality\",\n",
        "      \"improved production flexibility\", \"reduced production cost\",\n",
        "      \"improved yield or reduced material consumption\"])\n",
        "\n",
        "df_train['encoding'] = [model.encode(v) for v in df_train['sentence']]"
      ],
      "metadata": {
        "id": "j8uAZi7FjLDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[['key', 'sentence','encoding']].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "A2OBgU448VCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.join(pd.DataFrame(util.cos_sim(np.array(df_train['encoding'].values.tolist()), query_embedding)))"
      ],
      "metadata": {
        "id": "At7ZxDG22SXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(df_train, open('/content/drive/MyDrive/DCAST/Financial/Outputs/saved_thresholds/df_train_26Oct23v1.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "kRmcWKrPzUDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds_dict = {}\n",
        "for c in range(0, 8):\n",
        "  for s in range(0, 4):\n",
        "    thresholds_dict['tm'+str(s)+str(c)] = df_train[[c]].mean() + s*df_train[[c]].std()\n"
      ],
      "metadata": {
        "id": "MXcZYYSz2bwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(thresholds_dict, open('/content/drive/MyDrive/DCAST/Financial/Outputs/saved_thresholds/thresholds_main_model_26Oct23v1.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "Xmta4QFe3Mk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds_dict = pickle.load(open('/content/drive/MyDrive/DCAST/Financial/Outputs/saved_thresholds/thresholds_main_model_26Oct23v1.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "tyotC3fKzL2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classifier Component**"
      ],
      "metadata": {
        "id": "Bm8_rvFan0jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for multi-label classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import KFold, ShuffleSplit, StratifiedKFold, RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import ast\n",
        "from keras.optimizers import SGD\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "AFTVYRPKRwvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################reencode corpus\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util\n",
        "\n",
        "#model_name = 'all-mpnet-base-v2-withWeights-V7-2023-10-24_14-15'\n",
        "#model_save_path = '/content/drive/MyDrive/DCAST/Financial/Outputs/fine_tuned_models/all-mpnet-base-v2-withWeights-V7-2023-10-24_14-15'\n",
        "output_file_name = model_name + '.pickle'\n",
        "encoded_corpus_path = str(os.path.join(\"/content/drive/MyDrive/DCAST/Financial/Outputs/enconded_corpus\", output_file_name))\n"
      ],
      "metadata": {
        "id": "ruc4RhX84pln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalDF = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/7.finalDF_24Oct23.xlsx', index_col = 0)\n",
        "finalDF.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "D_HqrMtI2PWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode final df\n",
        "model = SentenceTransformer(model_save_path)\n",
        "finalDF['encoding'] = [model.encode(v) for v in finalDF['sentence']]"
      ],
      "metadata": {
        "id": "0E77paNk-0iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode ontology\n",
        "model = SentenceTransformer(model_save_path)\n",
        "query_embedding = model.encode([\"introduced a new generation of products\",\n",
        "      \"extended the product range\", \"opened new markets\",\n",
        "      \"entered a new technology field\" ,\"improved an existing product quality\",\n",
        "      \"improved production flexibility\", \"reduced production cost\",\n",
        "      \"improved yield or reduced material consumption\"])"
      ],
      "metadata": {
        "id": "X6-thza4-iiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalDF = finalDF.join(pd.DataFrame(util.cos_sim(np.array(finalDF['encoding'].values.tolist()), query_embedding)))"
      ],
      "metadata": {
        "id": "aQxveog4ALDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(finalDF, open('/content/drive/MyDrive/DCAST/Financial/Outputs/enconded_corpus/finalDFencoded_24Jan24v1.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "wMccLTaNBZr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalDF = pickle.load(open('/content/drive/MyDrive/DCAST/Financial/Outputs/enconded_corpus/finalDFencoded_24Jan24v1.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "XY5qOxbyqySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 8):\n",
        "  finalDF['ont'+str(i)] = finalDF.apply(lambda r: mean(query_embedding[i]), axis=1)"
      ],
      "metadata": {
        "id": "cwt7sxiTFzf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalDF['encoding'] = [mean(e) for e in finalDF['encoding']]"
      ],
      "metadata": {
        "id": "4QfJwp_oPVXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finalDF = finalDF[(finalDF['classAug']==0) | (finalDF['classAug']==7)]"
      ],
      "metadata": {
        "id": "mVtN3aX7SWuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "Z = np.array(finalDF['ontology']).reshape(-1, 1)\n",
        "enc.fit(Z)\n",
        "enc.categories_\n",
        "Z1 = enc.transform(Z)\n",
        "X_cos_sim = pd.DataFrame(finalDF.iloc[:,6:14]).to_numpy()\n",
        "X_encoding = pd.DataFrame(finalDF[['encoding']]).to_numpy()\n",
        "X_ontology = pd.DataFrame(finalDF.iloc[:,15:]).to_numpy()\n",
        "y = pd.DataFrame(Z1.toarray(), columns=enc.categories_).to_numpy()\n"
      ],
      "metadata": {
        "id": "HBLuDKviRyNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ontology = ['entered a new technology field', 'extended the product range', 'improved an existing product quality', 'improved production flexibility', 'improved yield or reduced material consumption', 'introduced a new generation of products', 'opened new markets', 'reduced production cost']"
      ],
      "metadata": {
        "id": "JR_eSgNKDLv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the model\n",
        "# from keras.metrics import binary_accuracy\n",
        "# def get_model(n_inputs, n_outputs):\n",
        "#   model = Sequential()\n",
        "#   model.add(Dense(8, input_dim=n_inputs, activation='relu'))\n",
        "#   model.add(Dense(n_outputs, activation='softmax'))\n",
        "#   #model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "#   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[binary_accuracy])\n",
        "#   return model\n",
        "\n",
        "# # evaluate a model using repeated k-fold cross-validation\n",
        "# def evaluate_model(X, y):\n",
        "#   results = list()\n",
        "#   n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "#   # define evaluation procedure\n",
        "#   #cv =  ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
        "#   cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
        "#   # enumerate folds\n",
        "#   for train, test in cv.split(X, y):\n",
        "#     # prepare data\n",
        "#     X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "#     # define model\n",
        "#     model = get_model(n_inputs, n_outputs)\n",
        "#     # fit model\n",
        "#     model.fit(X_train, y_train, epochs=20, batch_size=100, verbose=0)\n",
        "#     # make a prediction on the test set\n",
        "#     yhat = model.predict(X_test)\n",
        "#     # round probabilities to class labels\n",
        "#     yhat = yhat.round()\n",
        "#     #yhat = (yhat > 0.5)\n",
        "#     # calculate accuracy\n",
        "#     acc = f1_score(y_test, yhat, average='micro')\n",
        "#     # store result\n",
        "#     print('>%.3f' % acc)\n",
        "#     results.append(acc)\n",
        "#     return results, model, y[train], yhat\n",
        "\n",
        "\n",
        "# from keras.metrics import binary_accuracy\n",
        "# def get_model(n_inputs, n_outputs):\n",
        "#   model = Sequential()\n",
        "#   layer1 = Dense(1, input_dim=n_inputs, activation='relu')\n",
        "#   layer1.trainable=True\n",
        "#   model.add(layer1)\n",
        "#   layer2 = Dense(n_outputs, activation='sigmoid')\n",
        "#   layer2.trainable=False\n",
        "#   model.add(layer2)\n",
        "#   model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[binary_accuracy])\n",
        "#   return model\n",
        "\n",
        "from keras.metrics import binary_accuracy\n",
        "def get_model(n_inputs, n_outputs):\n",
        "  model = Sequential()\n",
        "\n",
        "  layer1 = Dense(1, input_dim=n_inputs, activation='tanh')\n",
        "  layer1.trainable=True\n",
        "  model.add(layer1)\n",
        "\n",
        "  #add cos_sim here\n",
        "  layer2 = Dense(1, input_dim=n_inputs, activation='tanh')\n",
        "  layer1.trainable=True\n",
        "  model.add(layer1)\n",
        "\n",
        "  #add attention layer (learn to pay more attention to the cosine similarity)\n",
        "\n",
        "  layer3 = Dense(n_outputs, activation='sigmoid')\n",
        "  layer3.trainable=False\n",
        "  model.add(layer2)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=[binary_accuracy])\n",
        "  return model\n",
        "\n",
        "#pass the mean of the encodings as inputs (size 3 input: mean_sentence_enc | mean_ontology_enc | cos_sim)\n",
        "#try other activation func (tanh, leaky relu, exponential relu, softplus)\n",
        "#try sgd instead of adam\n",
        "#try conditional random field (crf) - instead of NN\n",
        "#try 5, 10 epochs instead of 100\n",
        "\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y):\n",
        "  results = list()\n",
        "  n_inputs, n_outputs = 1, 1\n",
        "  # define evaluation procedure\n",
        "  #cv =  ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
        "  cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
        "  # enumerate folds\n",
        "  for train, test in cv.split(X, y):\n",
        "    # prepare data\n",
        "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "    # define model\n",
        "    model = get_model(n_inputs, n_outputs)\n",
        "    # fit model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=100, verbose=0)\n",
        "    # make a prediction on the test set\n",
        "    yhat = model.predict(X_test)\n",
        "    # round probabilities to class labels\n",
        "    yhat = yhat.round()\n",
        "    #yhat = (yhat > 0.5)\n",
        "    # calculate accuracy\n",
        "    acc = f1_score(y_test, yhat, average='micro')\n",
        "    # store result\n",
        "    print('>%.3f' % acc)\n",
        "    results.append(acc)\n",
        "    return results, model, y[train], yhat\n"
      ],
      "metadata": {
        "id": "nswi1nk6O5PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 epochs - F1 = 0.12\n",
        "25 epochs - F1 = 0.32, AUC = 0.67\n",
        "35 epochs - F1 = 0.26, AUC = 0.74\n",
        "50 epochs - F1 = 0.28, AUC = 0.77\n",
        "75 epochs - F1 = 0.25, AUC = 0.75\n",
        "\n",
        "Change training metric\n",
        "binary_accuracy - F1 = 0.32, AUC = 0.67\n",
        "BinaryCrossEntropy: F1 = 0.3, AUC = 0.69\n",
        "AUC: F1 = 0.25, AUC = 0.61\n",
        "\n",
        "Optimizer:\n",
        "adam - F1 = 0.32, AUC = 0.67\n",
        "sgd - F1 = 0.1 , AUC = 0.57\n",
        "\n",
        "\n",
        "Activation func:\n",
        "relu  - F1 = 0.32, AUC = 0.67\n",
        "tanh  - F1 = 0.24, AUC = 0.69\n",
        "leakyRelu - F1 = 0.27, AUC = 0.64\n",
        "ELU - F1 = 0.4, AUC = 0.66"
      ],
      "metadata": {
        "id": "xkxxIhr5Bd6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import binary_accuracy\n",
        "def get_model(n_inputs, n_outputs):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(1, input_dim=n_inputs, activation='ELU'))\n",
        "  model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[binary_accuracy])\n",
        "  return model\n",
        "\n",
        "#pass the mean of the encodings as inputs (size 3 input: mean_sentence_enc | mean_ontology_enc | cos_sim)\n",
        "#try other activation func (tanh, leaky relu, exponential relu, softplus)\n",
        "#try sgd instead of adam\n",
        "#try conditional random field (crf) - instead of NN\n",
        "#try 5, 10 epochs instead of 100\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y):\n",
        "  results = list()\n",
        "  n_inputs, n_outputs = X.shape[1], 1\n",
        "  # define evaluation procedure\n",
        "  #cv =  ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
        "  cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
        "  # enumerate folds\n",
        "  for train, test in cv.split(X, y):\n",
        "    # prepare data\n",
        "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "    # define model\n",
        "    model = get_model(n_inputs, n_outputs)\n",
        "    # fit model\n",
        "    model.fit(X_train, y_train, epochs=25, batch_size=100, verbose=0)\n",
        "    # make a prediction on the test set\n",
        "    yhat = model.predict(X_test)\n",
        "    # round probabilities to class labels\n",
        "    yhat = yhat.round()\n",
        "    #yhat = (yhat > 0.5)\n",
        "    # calculate accuracy\n",
        "    acc = f1_score(y_test, yhat, average='micro')\n",
        "    # store result\n",
        "    print('>%.3f' % acc)\n",
        "    results.append(acc)\n",
        "    return results, model, y[train], yhat\n"
      ],
      "metadata": {
        "id": "y_QbUddThzuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, o in enumerate(ontology):\n",
        "  X_cos_sim = pd.DataFrame(finalDF[finalDF['ontology']==o].iloc[:,6+i]).to_numpy()\n",
        "  X_encoding = pd.DataFrame(finalDF[finalDF['ontology']==o][['encoding']]).to_numpy()\n",
        "  X_ontology = np.array(finalDF[finalDF['ontology']==o].iloc[:,14+i].values.tolist()).reshape(-1,1)\n",
        "  X = np.hstack((X_cos_sim,X_encoding,X_ontology))\n",
        "  y = finalDF[finalDF['ontology']==o]['label'].to_numpy().reshape(-1,1)\n",
        "\n",
        "  # evaluate model\n",
        "  results_f1, model, ytrain, yhat = evaluate_model(X, y)\n",
        "  # summarize performance\n",
        "  print('F1: %.3f (%.3f)' % (mean(results_f1), std(results_f1)))\n",
        "\n",
        "  model.save('/content/drive/MyDrive/DCAST/Financial/Outputs/saved_classifiers/classifier_main_model_cat' + str(i) + '.h5')"
      ],
      "metadata": {
        "id": "7TS4TcPDDid-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save('/content/drive/MyDrive/DCAST/Financial/Outputs/saved_classifiers/classifier_main_model.h5')"
      ],
      "metadata": {
        "id": "UsC1-IdahsgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoding and Classifying test set**"
      ],
      "metadata": {
        "id": "XE8Vuy4eoBpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers --quiet"
      ],
      "metadata": {
        "id": "ik0dzS9YJOos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util\n",
        "import os"
      ],
      "metadata": {
        "id": "fvk9fYgeoBKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/8.Test_Set_Sentences_13Oct23_v1aos.xlsx')"
      ],
      "metadata": {
        "id": "XTNamimbmuMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode test set\n",
        "model_name = 'all-mpnet-base-v2-withWeights-V7-2024-01-25_00-00'\n",
        "model_save_path = '/content/drive/MyDrive/DCAST/Financial/Outputs/fine_tuned_models/' + model_name\n",
        "\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "query_embedding = model.encode([\"introduced a new generation of products\",\n",
        "      \"extended the product range\", \"opened new markets\",\n",
        "      \"entered a new technology field\" ,\"improved an existing product quality\",\n",
        "      \"improved production flexibility\", \"reduced production cost\",\n",
        "      \"improved yield or reduced material consumption\"])\n",
        "\n",
        "test['encoding'] = [model.encode(v) for v in test['sentence']]\n"
      ],
      "metadata": {
        "id": "V-Qkf_99Ie4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.join(pd.DataFrame(util.cos_sim(np.array(test['encoding'].values.tolist()), query_embedding)))"
      ],
      "metadata": {
        "id": "E4IOCBGpl5Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Tr734srj-j22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(test, open('/content/drive/MyDrive/DCAST/Financial/Outputs/encoded_test_set/' + model_name + '.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "lGRkEfCvfHM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'all-mpnet-base-v2-withWeights-V7-2024-01-25_00-00'\n",
        "test = pickle.load(open('/content/drive/MyDrive/DCAST/Financial/Outputs/encoded_test_set/' + model_name + '.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "T-uKIGcnj7A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 8):\n",
        "  test['ont'+str(i)] = test.apply(lambda r: mean(query_embedding[i]), axis=1)"
      ],
      "metadata": {
        "id": "zm5UTCzQK_yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['encoding'] = [mean(e) for e in test['encoding']]"
      ],
      "metadata": {
        "id": "5fwuJhqoK_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import classifier\n",
        "from keras.models import load_model\n",
        "\n",
        "df = test\n",
        "\n",
        "for i in range(0, 8):\n",
        "  # Load model to classify category i\n",
        "  model2 = load_model('/content/drive/MyDrive/DCAST/Financial/Outputs/saved_classifiers/classifier_main_model_cat' + str(i) + '.h5')\n",
        "  # Define X\n",
        "  X_test_cos_sim = pd.DataFrame(test.iloc[:,6+i]).to_numpy()\n",
        "  X_test_encoding = pd.DataFrame(test[['encoding']]).to_numpy()\n",
        "  X_test_ontology = np.array(test.iloc[:,14+i].values.tolist()).reshape(-1,1)\n",
        "  X_test = np.hstack((X_test_cos_sim,X_test_encoding,X_test_ontology))\n",
        "  # Predict\n",
        "  yhat = model2.predict(X_test)\n",
        "  #Merge with df\n",
        "  df[\"c\"+str(i+1)] = yhat\n"
      ],
      "metadata": {
        "id": "Ldpo3xvWPbwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = test.join(pd.DataFrame(yhat, columns=['c1', 'c2', 'c3', 'c4', 'c5' ,'c6' , 'c7' ,'c8']))"
      ],
      "metadata": {
        "id": "2pPIugWsQvMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['key', 'sentence', 'c1', 'c2', 'c3', 'c4', 'c5' ,'c6' , 'c7' ,'c8']]\n",
        "df1 = pd.melt(df, id_vars = ['key', 'sentence'], var_name='category', value_name='score')\n",
        "df1 = df1.iloc[:,[0,2,3]]\n",
        "df1 = df1.groupby(['key', 'category'], as_index=False).max()\n",
        "manual = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/9.Final_labels_agreement_firm_level_13Oct23_v1.xlsx', names=['key', 'c1', 'c2', 'c3', 'c4', 'c5' ,'c6' , 'c7' ,'c8'])\n",
        "manual = pd.melt(manual, id_vars='key', var_name='category', value_name='score')\n",
        "df2 = df1.merge(manual, on=['key', 'category'])\n",
        "df2"
      ],
      "metadata": {
        "id": "yl6jMFjkwZx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "\n",
        "overall = pd.DataFrame(columns=['Model Name', 'Weighted F-1', 'Weighted Precision', 'Weighted Recall' , 'Weighted Accuracy', 'AUC'])\n",
        "f1_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "precision_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "recall_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "accuracy_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "aucdf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "\n",
        "def stat_threshold(pred, y):\n",
        "  threshold = 0.5\n",
        "  y_pred = [1 if x >= threshold else 0 for x in pred]\n",
        "  f1 = metrics.f1_score(y, y_pred, average='binary')\n",
        "  return[f1, threshold, y_pred]\n",
        "\n",
        "#class1\n",
        "\n",
        "y1 = df2[df2.category=='c1']['score_y']\n",
        "pred1 = df2[df2.category=='c1']['score_x']\n",
        "fpr1, tpr1, thresholds1 = metrics.roc_curve(y1, pred1)\n",
        "f1, t1, y_pred1 = stat_threshold(pred1, y1)\n",
        "\n",
        "#class2\n",
        "\n",
        "y2 = df2[df2.category=='c2']['score_y']\n",
        "pred2 = df2[df2.category=='c2']['score_x']\n",
        "fpr2, tpr2, thresholds2 = metrics.roc_curve(y2, pred2)\n",
        "f1, t2, y_pred2 = stat_threshold(pred2, y2)\n",
        "\n",
        "#class3\n",
        "\n",
        "y3 = df2[df2.category=='c3']['score_y']\n",
        "pred3 = df2[df2.category=='c3']['score_x']\n",
        "fpr3, tpr3, thresholds3 = metrics.roc_curve(y3, pred3)\n",
        "f1, t3, y_pred3 = stat_threshold(pred3, y3)\n",
        "\n",
        "#class4\n",
        "\n",
        "y4 = df2[df2.category=='c4']['score_y']\n",
        "pred4 = df2[df2.category=='c4']['score_x']\n",
        "fpr4, tpr4, thresholds4 = metrics.roc_curve(y4, pred4)\n",
        "f1, t4, y_pred4 = stat_threshold(pred4, y4)\n",
        "\n",
        "#class5\n",
        "\n",
        "y5 = df2[df2.category=='c5']['score_y']\n",
        "pred5 = df2[df2.category=='c5']['score_x']\n",
        "fpr5, tpr5, thresholds5 = metrics.roc_curve(y5, pred5)\n",
        "f1, t5, y_pred5 = stat_threshold(pred5, y5)\n",
        "\n",
        "#class6\n",
        "\n",
        "y6 = df2[df2.category=='c6']['score_y']\n",
        "pred6 = df2[df2.category=='c6']['score_x']\n",
        "fpr6, tpr6, thresholds6 = metrics.roc_curve(y6, pred6)\n",
        "f1, t6, y_pred6 = stat_threshold(pred6, y6)\n",
        "\n",
        "#class7\n",
        "\n",
        "y7 = df2[df2.category=='c7']['score_y']\n",
        "pred7 = df2[df2.category=='c7']['score_x']\n",
        "fpr7, tpr7, thresholds7 = metrics.roc_curve(y7, pred7)\n",
        "f1, t7, y_pred7 = stat_threshold(pred7, y7)\n",
        "\n",
        "#class8\n",
        "\n",
        "y8 = df2[df2.category=='c8']['score_y']\n",
        "pred8 = df2[df2.category=='c8']['score_x']\n",
        "fpr8, tpr8, thresholds8 = metrics.roc_curve(y8, pred8)\n",
        "f1, t8, y_pred8 = stat_threshold(pred8, y8)\n",
        "\n",
        "#Weighted F1-score\n",
        "\n",
        "wf1 = ((sum(y1)*metrics.f1_score(y1, y_pred1, average='binary')+ \\\n",
        "sum(y2)*metrics.f1_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.f1_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.f1_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.f1_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.f1_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.f1_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.f1_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted Precision\n",
        "wp = ((sum(y1)*metrics.precision_score(y1, y_pred1, average='binary')+\n",
        "sum(y2)*metrics.precision_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.precision_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.precision_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.precision_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.precision_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.precision_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.precision_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted Recall\n",
        "wr = ((sum(y1)*metrics.recall_score(y1, y_pred1, average='binary')+\n",
        "sum(y2)*metrics.recall_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.recall_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.recall_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.recall_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.recall_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.recall_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.recall_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted accuracy\n",
        "wacc = ((sum(y1)*metrics.accuracy_score(y1, y_pred1)+\n",
        "sum(y2)*metrics.accuracy_score(y2, y_pred2)+ \\\n",
        "sum(y3)*metrics.accuracy_score(y3, y_pred3)+ \\\n",
        "sum(y4)*metrics.accuracy_score(y4, y_pred4)+ \\\n",
        "sum(y5)*metrics.accuracy_score(y5, y_pred5)+ \\\n",
        "sum(y6)*metrics.accuracy_score(y6, y_pred6)+ \\\n",
        "sum(y7)*metrics.accuracy_score(y7, y_pred7)+ \\\n",
        "sum(y8)*metrics.accuracy_score(y8, y_pred8))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#AUC\n",
        "auc = ((sum(y1)*metrics.auc(fpr1, tpr1)+\n",
        "sum(y2)*metrics.auc(fpr2, tpr2)+ \\\n",
        "sum(y3)*metrics.auc(fpr3, tpr3)+ \\\n",
        "sum(y4)*metrics.auc(fpr4, tpr4)+ \\\n",
        "sum(y5)*metrics.auc(fpr5, tpr5)+ \\\n",
        "sum(y6)*metrics.auc(fpr6, tpr6)+ \\\n",
        "sum(y7)*metrics.auc(fpr7, tpr7)+ \\\n",
        "sum(y8)*metrics.auc(fpr8, tpr8))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "i=1\n",
        "overall.at[i, 'Model Name'] = 'main'\n",
        "overall.at[i, 'Weighted F-1'] = wf1\n",
        "overall.at[i, 'Weighted Precision'] = wp\n",
        "overall.at[i, 'Weighted Recall'] = wr\n",
        "overall.at[i, 'Weighted Accuracy'] = wacc\n",
        "overall.at[i, 'AUC'] = auc\n",
        "\n",
        "for dataf in ['f1_score', 'precision_score', 'recall_score', 'accuracy_score', 'auc']:\n",
        "  globals()[dataf + 'df'].at[i, 'Metric'] = dataf\n",
        "  globals()[dataf + 'df'].at[i, 'Model'] = 'main'\n",
        "  for cat in range(1,9):\n",
        "    if dataf in ['f1_score', 'precision_score', 'recall_score']:\n",
        "      globals()[dataf + 'df'].at[i, 'c'+ str(cat)] = globals()[dataf](globals()['y'+str(cat)], globals()['y_pred'+str(cat)], average='binary')\n",
        "    elif dataf == 'accuracy':\n",
        "      globals()[dataf + 'df'].at[i, 'c'+ str(cat)] = globals()[dataf](globals()['y'+str(cat)], globals()['y_pred'+str(cat)])\n",
        "    else:\n",
        "      globals()[dataf + 'df'].at[i, 'c'+ str(cat)] = metrics.auc(globals()['fpr'+str(cat)], globals()['tpr'+str(cat)])\n",
        "\n"
      ],
      "metadata": {
        "id": "QSe9OGE_bLd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall"
      ],
      "metadata": {
        "id": "w5lb0x7-d8iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scoredf"
      ],
      "metadata": {
        "id": "Mgx9IHbcdRxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####"
      ],
      "metadata": {
        "id": "Fiqyh0KkADfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pickle.load(open('/content/drive/MyDrive/DCAST/Financial/Outputs/encoded_test_set/' + model_name + '.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "QJ0mvdnTAEyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.DataFrame(test.iloc[:,4:]).to_numpy()"
      ],
      "metadata": {
        "id": "50ctZlt9AEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.rename(columns={0:'c1', 1:'c2', 2:'c3', 3:'c4', 4:'c5', 5:'c6', 6:'c7', 7:'c8'})"
      ],
      "metadata": {
        "id": "spo49tB1AfP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = test[['key', 'sentence', 'c1', 'c2', 'c3', 'c4', 'c5' ,'c6' , 'c7' ,'c8']]"
      ],
      "metadata": {
        "id": "WKpdOAd8AEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.melt(df, id_vars = ['key', 'sentence'], var_name='category', value_name='score')\n",
        "df1 = df.iloc[:,[0,2,3]]\n",
        "df1 = df1.groupby(['key', 'category'], as_index=False).max()"
      ],
      "metadata": {
        "id": "wWPIJESLAEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/9.Final_labels_agreement_firm_level_13Oct23_v1.xlsx', names=['key', 'c1', 'c2', 'c3', 'c4', 'c5' ,'c6' , 'c7' ,'c8'])"
      ],
      "metadata": {
        "id": "DJA-F8wNAEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual = pd.melt(manual, id_vars='key', var_name='category', value_name='score')\n",
        "df2 = df1.merge(manual, on=['key', 'category'])"
      ],
      "metadata": {
        "id": "7Oq4uvFqAEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[df2['category']=='c1']"
      ],
      "metadata": {
        "id": "Lwn3xItoAEyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "\n",
        "def stat_threshold(pred, y):\n",
        "  threshold = np.mean(df.score) + 3*np.std(df.score)\n",
        "  y_pred = [1 if x >= threshold else 0 for x in pred]\n",
        "  f1 = metrics.f1_score(y, y_pred, average='binary')\n",
        "  return[f1, threshold, y_pred]\n",
        "\n",
        "#class1\n",
        "\n",
        "y1 = df2[df2.category=='c1']['score_y']\n",
        "pred1 = df2[df2.category=='c1']['score_x']\n",
        "fpr1, tpr1, thresholds1 = metrics.roc_curve(y1, pred1)\n",
        "f1, t1, y_pred1 = stat_threshold(pred1, y1)\n",
        "\n",
        "#class2\n",
        "\n",
        "y2 = df2[df2.category=='c2']['score_y']\n",
        "pred2 = df2[df2.category=='c2']['score_x']\n",
        "fpr2, tpr2, thresholds2 = metrics.roc_curve(y2, pred2)\n",
        "f1, t2, y_pred2 = stat_threshold(pred2, y2)\n",
        "\n",
        "#class3\n",
        "\n",
        "y3 = df2[df2.category=='c3']['score_y']\n",
        "pred3 = df2[df2.category=='c3']['score_x']\n",
        "fpr3, tpr3, thresholds3 = metrics.roc_curve(y3, pred3)\n",
        "f1, t3, y_pred3 = stat_threshold(pred3, y3)\n",
        "\n",
        "#class4\n",
        "\n",
        "y4 = df2[df2.category=='c4']['score_y']\n",
        "pred4 = df2[df2.category=='c4']['score_x']\n",
        "fpr4, tpr4, thresholds4 = metrics.roc_curve(y4, pred4)\n",
        "f1, t4, y_pred4 = stat_threshold(pred4, y4)\n",
        "\n",
        "#class5\n",
        "\n",
        "y5 = df2[df2.category=='c5']['score_y']\n",
        "pred5 = df2[df2.category=='c5']['score_x']\n",
        "fpr5, tpr5, thresholds5 = metrics.roc_curve(y5, pred5)\n",
        "f1, t5, y_pred5 = stat_threshold(pred5, y5)\n",
        "\n",
        "#class6\n",
        "\n",
        "y6 = df2[df2.category=='c6']['score_y']\n",
        "pred6 = df2[df2.category=='c6']['score_x']\n",
        "fpr6, tpr6, thresholds6 = metrics.roc_curve(y6, pred6)\n",
        "f1, t6, y_pred6 = stat_threshold(pred6, y6)\n",
        "\n",
        "#class7\n",
        "\n",
        "y7 = df2[df2.category=='c7']['score_y']\n",
        "pred7 = df2[df2.category=='c7']['score_x']\n",
        "fpr7, tpr7, thresholds7 = metrics.roc_curve(y7, pred7)\n",
        "f1, t7, y_pred7 = stat_threshold(pred7, y7)\n",
        "\n",
        "#class8\n",
        "\n",
        "y8 = df2[df2.category=='c8']['score_y']\n",
        "pred8 = df2[df2.category=='c8']['score_x']\n",
        "fpr8, tpr8, thresholds8 = metrics.roc_curve(y8, pred8)\n",
        "f1, t8, y_pred8 = stat_threshold(pred8, y8)\n",
        "\n",
        "#Weighted F1-score\n",
        "\n",
        "wf1 = ((sum(y1)*metrics.f1_score(y1, y_pred1, average='binary')+ \\\n",
        "sum(y2)*metrics.f1_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.f1_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.f1_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.f1_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.f1_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.f1_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.f1_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted Precision\n",
        "wp = ((sum(y1)*metrics.precision_score(y1, y_pred1, average='binary')+\n",
        "sum(y2)*metrics.precision_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.precision_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.precision_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.precision_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.precision_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.precision_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.precision_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted Recall\n",
        "wr = ((sum(y1)*metrics.recall_score(y1, y_pred1, average='binary')+\n",
        "sum(y2)*metrics.recall_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.recall_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.recall_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.recall_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.recall_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.recall_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.recall_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted accuracy\n",
        "wacc = ((sum(y1)*metrics.accuracy_score(y1, y_pred1)+\n",
        "sum(y2)*metrics.accuracy_score(y2, y_pred2)+ \\\n",
        "sum(y3)*metrics.accuracy_score(y3, y_pred3)+ \\\n",
        "sum(y4)*metrics.accuracy_score(y4, y_pred4)+ \\\n",
        "sum(y5)*metrics.accuracy_score(y5, y_pred5)+ \\\n",
        "sum(y6)*metrics.accuracy_score(y6, y_pred6)+ \\\n",
        "sum(y7)*metrics.accuracy_score(y7, y_pred7)+ \\\n",
        "sum(y8)*metrics.accuracy_score(y8, y_pred8))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#AUC\n",
        "auc = ((sum(y1)*metrics.auc(fpr1, tpr1)+\n",
        "sum(y2)*metrics.auc(fpr2, tpr2)+ \\\n",
        "sum(y3)*metrics.auc(fpr3, tpr3)+ \\\n",
        "sum(y4)*metrics.auc(fpr4, tpr4)+ \\\n",
        "sum(y5)*metrics.auc(fpr5, tpr5)+ \\\n",
        "sum(y6)*metrics.auc(fpr6, tpr6)+ \\\n",
        "sum(y7)*metrics.auc(fpr7, tpr7)+ \\\n",
        "sum(y8)*metrics.auc(fpr8, tpr8))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Bj2TC9OAEyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall = pd.DataFrame(columns=['Model Name', 'Weighted F-1', 'Weighted Precision', 'Weighted Recall' , 'Weighted Accuracy', 'AUC'])\n",
        "\n",
        "i=1\n",
        "overall.at[i, 'Model Name'] = 'main'\n",
        "overall.at[i, 'Weighted F-1'] = wf1\n",
        "overall.at[i, 'Weighted Precision'] = wp\n",
        "overall.at[i, 'Weighted Recall'] = wr\n",
        "overall.at[i, 'Weighted Accuracy'] = wacc\n",
        "overall.at[i, 'AUC'] = auc"
      ],
      "metadata": {
        "id": "w6mtprw2AEyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall"
      ],
      "metadata": {
        "id": "KGUzoP8RAEyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thresholds using the training dataset"
      ],
      "metadata": {
        "id": "dz3IrVdzGMt9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIXAIN1dGMRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test[['key', 'sentence', 0,1,2,3,4,5,6,7]]\n",
        "test = test.rename(columns={0:'c1', 1:'c2', 2:'c3', 3:'c4', 4:'c5', 5:'c6', 6:'c7', 7:'c8'})"
      ],
      "metadata": {
        "id": "jvCHBtMs95VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.melt(test, id_vars = ['key', 'sentence'], var_name='category', value_name='score')\n",
        "df1 = df.iloc[:,[0,2,3]]\n",
        "df1 = df1.groupby(['key', 'category'], as_index=False).max()"
      ],
      "metadata": {
        "id": "KsNnlPO-9uB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual = pd.read_excel('/content/drive/MyDrive/DCAST/Financial/Inputs/9.Final_labels_agreement_firm_level_13Oct23_v1.xlsx', names=['key', 'c1', 'c2', 'c3', 'c4', 'c5' ,'c6' , 'c7' ,'c8'])"
      ],
      "metadata": {
        "id": "kC_gvNuh9yCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual = pd.melt(manual, id_vars='key', var_name='category', value_name='score')\n",
        "df2 = df1.merge(manual, on=['key', 'category'])"
      ],
      "metadata": {
        "id": "LA-J1OeI91A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "\n",
        "overall = pd.DataFrame(columns=['Model Name', 'Weighted F-1', 'Weighted Precision', 'Weighted Recall' , 'Weighted Accuracy', 'AUC'])\n",
        "f1_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "precision_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "recall_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "accuracy_scoredf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "aucdf = pd.DataFrame(columns= ['Metric','Model','c1','c2','c3','c4','c5','c6','c7','c8'])\n",
        "\n",
        "def stat_threshold(pred, y, key):\n",
        "  threshold = thresholds_dict[key].values\n",
        "  y_pred = [1 if x >= threshold else 0 for x in pred]\n",
        "  f1 = metrics.f1_score(y, y_pred, average='binary')\n",
        "  return[f1, threshold, y_pred]\n",
        "\n",
        "#class1\n",
        "\n",
        "y1 = df2[df2.category=='c1']['score_y']\n",
        "pred1 = df2[df2.category=='c1']['score_x']\n",
        "fpr1, tpr1, thresholds1 = metrics.roc_curve(y1, pred1)\n",
        "f1, t1, y_pred1 = stat_threshold(pred1, y1, 'tm30')\n",
        "\n",
        "#class2\n",
        "\n",
        "y2 = df2[df2.category=='c2']['score_y']\n",
        "pred2 = df2[df2.category=='c2']['score_x']\n",
        "fpr2, tpr2, thresholds2 = metrics.roc_curve(y2, pred2)\n",
        "f1, t2, y_pred2 = stat_threshold(pred2, y2, 'tm31')\n",
        "\n",
        "#class3\n",
        "\n",
        "y3 = df2[df2.category=='c3']['score_y']\n",
        "pred3 = df2[df2.category=='c3']['score_x']\n",
        "fpr3, tpr3, thresholds3 = metrics.roc_curve(y3, pred3)\n",
        "f1, t3, y_pred3 = stat_threshold(pred3, y3, 'tm32')\n",
        "\n",
        "#class4\n",
        "\n",
        "y4 = df2[df2.category=='c4']['score_y']\n",
        "pred4 = df2[df2.category=='c4']['score_x']\n",
        "fpr4, tpr4, thresholds4 = metrics.roc_curve(y4, pred4)\n",
        "f1, t4, y_pred4 = stat_threshold(pred4, y4, 'tm33')\n",
        "\n",
        "#class5\n",
        "\n",
        "y5 = df2[df2.category=='c5']['score_y']\n",
        "pred5 = df2[df2.category=='c5']['score_x']\n",
        "fpr5, tpr5, thresholds5 = metrics.roc_curve(y5, pred5)\n",
        "f1, t5, y_pred5 = stat_threshold(pred5, y5, 'tm34')\n",
        "\n",
        "#class6\n",
        "\n",
        "y6 = df2[df2.category=='c6']['score_y']\n",
        "pred6 = df2[df2.category=='c6']['score_x']\n",
        "fpr6, tpr6, thresholds6 = metrics.roc_curve(y6, pred6)\n",
        "f1, t6, y_pred6 = stat_threshold(pred6, y6, 'tm35')\n",
        "\n",
        "#class7\n",
        "\n",
        "y7 = df2[df2.category=='c7']['score_y']\n",
        "pred7 = df2[df2.category=='c7']['score_x']\n",
        "fpr7, tpr7, thresholds7 = metrics.roc_curve(y7, pred7)\n",
        "f1, t7, y_pred7 = stat_threshold(pred7, y7, 'tm36')\n",
        "\n",
        "#class8\n",
        "\n",
        "y8 = df2[df2.category=='c8']['score_y']\n",
        "pred8 = df2[df2.category=='c8']['score_x']\n",
        "fpr8, tpr8, thresholds8 = metrics.roc_curve(y8, pred8)\n",
        "f1, t8, y_pred8 = stat_threshold(pred8, y8, 'tm37')\n",
        "\n",
        "#Weighted F1-score\n",
        "\n",
        "wf1 = ((sum(y1)*metrics.f1_score(y1, y_pred1, average='binary')+ \\\n",
        "sum(y2)*metrics.f1_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.f1_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.f1_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.f1_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.f1_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.f1_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.f1_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted Precision\n",
        "wp = ((sum(y1)*metrics.precision_score(y1, y_pred1, average='binary')+\n",
        "sum(y2)*metrics.precision_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.precision_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.precision_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.precision_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.precision_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.precision_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.precision_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted Recall\n",
        "wr = ((sum(y1)*metrics.recall_score(y1, y_pred1, average='binary')+\n",
        "sum(y2)*metrics.recall_score(y2, y_pred2, average='binary')+ \\\n",
        "sum(y3)*metrics.recall_score(y3, y_pred3, average='binary')+ \\\n",
        "sum(y4)*metrics.recall_score(y4, y_pred4, average='binary')+ \\\n",
        "sum(y5)*metrics.recall_score(y5, y_pred5, average='binary')+ \\\n",
        "sum(y6)*metrics.recall_score(y6, y_pred6, average='binary')+ \\\n",
        "sum(y7)*metrics.recall_score(y7, y_pred7, average='binary')+ \\\n",
        "sum(y8)*metrics.recall_score(y8, y_pred8, average='binary'))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#Weighted accuracy\n",
        "wacc = ((sum(y1)*metrics.accuracy_score(y1, y_pred1)+\n",
        "sum(y2)*metrics.accuracy_score(y2, y_pred2)+ \\\n",
        "sum(y3)*metrics.accuracy_score(y3, y_pred3)+ \\\n",
        "sum(y4)*metrics.accuracy_score(y4, y_pred4)+ \\\n",
        "sum(y5)*metrics.accuracy_score(y5, y_pred5)+ \\\n",
        "sum(y6)*metrics.accuracy_score(y6, y_pred6)+ \\\n",
        "sum(y7)*metrics.accuracy_score(y7, y_pred7)+ \\\n",
        "sum(y8)*metrics.accuracy_score(y8, y_pred8))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "#AUC\n",
        "auc = ((sum(y1)*metrics.auc(fpr1, tpr1)+\n",
        "sum(y2)*metrics.auc(fpr2, tpr2)+ \\\n",
        "sum(y3)*metrics.auc(fpr3, tpr3)+ \\\n",
        "sum(y4)*metrics.auc(fpr4, tpr4)+ \\\n",
        "sum(y5)*metrics.auc(fpr5, tpr5)+ \\\n",
        "sum(y6)*metrics.auc(fpr6, tpr6)+ \\\n",
        "sum(y7)*metrics.auc(fpr7, tpr7)+ \\\n",
        "sum(y8)*metrics.auc(fpr8, tpr8))/ \\\n",
        "(sum(y1) + sum(y2) + sum(y3) + sum(y4)+ \\\n",
        "sum(y5) + sum(y6) + sum(y7) + sum(y8)))\n",
        "\n",
        "i=1\n",
        "overall.at[i, 'Model Name'] = 'main'\n",
        "overall.at[i, 'Weighted F-1'] = wf1\n",
        "overall.at[i, 'Weighted Precision'] = wp\n",
        "overall.at[i, 'Weighted Recall'] = wr\n",
        "overall.at[i, 'Weighted Accuracy'] = wacc\n",
        "overall.at[i, 'AUC'] = auc\n",
        "\n",
        "for dataf in ['f1_score', 'precision_score', 'recall_score', 'accuracy_score', 'auc']:\n",
        "  globals()[dataf + 'df'].at[i, 'Metric'] = dataf\n",
        "  globals()[dataf + 'df'].at[i, 'Model'] = 'main'\n",
        "  for cat in range(1,9):\n",
        "    if dataf in ['f1_score', 'precision_score', 'recall_score']:\n",
        "      globals()[dataf + 'df'].at[i, 'c'+ str(cat)] = globals()[dataf](globals()['y'+str(cat)], globals()['y_pred'+str(cat)], average='binary')\n",
        "    elif dataf == 'accuracy':\n",
        "      globals()[dataf + 'df'].at[i, 'c'+ str(cat)] = globals()[dataf](globals()['y'+str(cat)], globals()['y_pred'+str(cat)])\n",
        "    else:\n",
        "      globals()[dataf + 'df'].at[i, 'c'+ str(cat)] = metrics.auc(globals()['fpr'+str(cat)], globals()['tpr'+str(cat)])\n",
        "\n"
      ],
      "metadata": {
        "id": "qy_FwcRW4bsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall"
      ],
      "metadata": {
        "id": "J3aq4Zyy5LUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scoredf"
      ],
      "metadata": {
        "id": "62aQYKOB5M2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aucdf"
      ],
      "metadata": {
        "id": "ihSAPsJC_QLg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}