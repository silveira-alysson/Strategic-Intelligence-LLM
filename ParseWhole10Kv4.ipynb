{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CLqSMmOUzz_YY8vtaVMIOs_ih1mXdFx1",
      "authorship_tag": "ABX9TyPtlXeW0/AgL+BWtMT5M6Eq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silveira-alysson/Strategic-Intelligence-LLM/blob/main/ParseWhole10Kv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu1xTglhDe0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70606f7d-ecaa-443d-b83f-dc316bfc1938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import unicodedata\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def parse10k(file):\n",
        "  temp_dict = dict()\n",
        "  try:\n",
        "    soup = BeautifulSoup(open(file), 'lxml')\n",
        "    for table in soup.find_all(\"table\"):\n",
        "        table.extract()\n",
        "    for filing_document in soup.find_all(\n",
        "            'document'):  # The document tags contain the various components of the total 10K filing pack\n",
        "\n",
        "        # The 'type' tag contains the document type\n",
        "        document_type = filing_document.type.find(text=True, recursive=False).strip()\n",
        "\n",
        "\n",
        "        if document_type == \"10-K\":  # Once the 10K text body is found\n",
        "\n",
        "            # Grab and store the 10K text body\n",
        "\n",
        "            TenKtext = filing_document.find('text').extract().text\n",
        "            TenKtext = unicodedata.normalize(\"NFKD\", TenKtext)\n",
        "\n",
        "            # Clean newly extracted text\n",
        "            TenKItem = TenKtext.strip()  # Remove starting/ending white spaces\n",
        "            TenKItem = TenKItem.replace('\\n', ' ')  # Replace \\n (new line) with space\n",
        "            TenKItem = TenKItem.replace('\\r', '')  # Replace \\r (carriage returns-if you're on windows) with space\n",
        "            TenKItem = TenKItem.replace(' ', ' ')  # Replace \" \" (a special character for space in HTML) with space\n",
        "            TenKItem = TenKItem.replace(' ', ' ')  # Replace \" \" (a special character for space in HTML) with space\n",
        "            while '  ' in TenKItem:\n",
        "                TenKItem = TenKItem.replace('  ', ' ')  # Remove extra spaces\n",
        "            namef = file[-24:-4]\n",
        "            #temp_dict = {file[-24:-4]:tokenize.sent_tokenize(TenKItem)}\n",
        "  except:\n",
        "    TenKItem = \"\"\n",
        "    namef = file[-24:-4]\n",
        "    #temp_dict[file[-24:-4]]= \"\"\n",
        "  return [namef, TenKItem]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  files = []\n",
        "  sent_corpus = dict()\n",
        "\n",
        "  input_path = '/content/drive/MyDrive/AssetPricingPaper/Inputs/'\n",
        "  output_path = '/content/drive/MyDrive/AssetPricingPaper/Outputs/'\n",
        "  files = [os.path.join(input_path,filename) for filename in os.listdir(input_path)]\n",
        "  processes = cpu_count()\n",
        "  chunk_size = int(len(files) // (processes**2) + 1)\n",
        "  pool = Pool(processes)\n",
        "  results = pool.imap_unordered(parse10k, files)\n",
        "  for result in results:\n",
        "    filename = os.path.join(output_path, result[0])\n",
        "    with(open(filename+\".txt\", \"w\")) as f:\n",
        "      f.write(result[1])\n"
      ]
    }
  ]
}